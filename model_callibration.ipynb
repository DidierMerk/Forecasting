{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a30027-c7cd-4388-b4a4-64e088426089",
   "metadata": {},
   "source": [
    "# Timeseries Forecasting on Transaction Data\n",
    "\n",
    "## 1. Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f370eac-e59d-483a-bbcb-d8b8c00c6277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:30.047609Z",
     "iopub.status.busy": "2024-10-31T13:35:30.046873Z",
     "iopub.status.idle": "2024-10-31T13:35:30.053216Z",
     "shell.execute_reply": "2024-10-31T13:35:30.051911Z",
     "shell.execute_reply.started": "2024-10-31T13:35:30.047562Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install datasetsforecast\n",
    "# !pip install entropyhub\n",
    "# !pip install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba4dd0-0d46-4cb2-91ed-558c91d258e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:30.373180Z",
     "iopub.status.busy": "2024-10-31T13:35:30.372486Z",
     "iopub.status.idle": "2024-10-31T13:35:30.380606Z",
     "shell.execute_reply": "2024-10-31T13:35:30.379082Z",
     "shell.execute_reply.started": "2024-10-31T13:35:30.373125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Some functions for plotting and stuff\n",
    "import ts_utils as ts_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4d18c-f64f-4963-8888-3809aadf64a6",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fd7b5-e846-4278-a63f-619d035eacc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:30.604114Z",
     "iopub.status.busy": "2024-10-31T13:35:30.603457Z",
     "iopub.status.idle": "2024-10-31T13:35:30.705360Z",
     "shell.execute_reply": "2024-10-31T13:35:30.704448Z",
     "shell.execute_reply.started": "2024-10-31T13:35:30.604071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Size of the data to read\n",
    "data_size = 'norm'\n",
    "\n",
    "# Date of the data to read\n",
    "data_date = '2110' # '2110' = 21st of October\n",
    "\n",
    "# Read the data (takes around 2 minutes)\n",
    "dataset = pd.read_csv(f\"~/Thesis/data/eod_balances_{data_date}_{data_size}.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14a5a4-7e96-4554-abc4-8fefb9938de9",
   "metadata": {},
   "source": [
    "### 2.1 In-sample and Out-sample split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c263461-2ec8-4879-a050-4a91f79e6c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.044316Z",
     "iopub.status.busy": "2024-10-31T13:35:32.043543Z",
     "iopub.status.idle": "2024-10-31T13:35:32.054262Z",
     "shell.execute_reply": "2024-10-31T13:35:32.053084Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.044274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total amount of timeseries\n",
    "num_timeseries = len(dataset.columns) - 1\n",
    "\n",
    "# Specify train test split percentage\n",
    "train_test_split = 0.8\n",
    "\n",
    "# Split into train and out of sample test data\n",
    "num_out_of_sample = int(train_test_split * num_timeseries)\n",
    "\n",
    "# Create in-sample dataframe\n",
    "in_sample_data = dataset.iloc[:, : num_out_of_sample + 1] # Training and testing\n",
    "\n",
    "# Create out-sample dataframe\n",
    "n = num_timeseries-num_out_of_sample\n",
    "columns_to_keep = dataset.columns[[0]].tolist() + dataset.columns[-n:].tolist()\n",
    "out_sample_data = dataset[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391ab1e-7035-43bd-8582-db5500ea3630",
   "metadata": {},
   "source": [
    "## 3. In-sample analysis\n",
    "\n",
    "### 3.1 Train/Test splitting and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd8228-3a76-4697-90f3-212ef8a378f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.056064Z",
     "iopub.status.busy": "2024-10-31T13:35:32.055686Z",
     "iopub.status.idle": "2024-10-31T13:35:32.121966Z",
     "shell.execute_reply": "2024-10-31T13:35:32.120818Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.056039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the data to the long format\n",
    "Y_df = in_sample_data.melt(id_vars=['date'], var_name='unique_id', value_name='y')\n",
    "Y_df = Y_df.rename(columns={'date':'ds'})\n",
    "\n",
    "# Convert date column to datetime type\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea34c1e-1b53-4a12-8b2b-f766d6073fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.123046Z",
     "iopub.status.busy": "2024-10-31T13:35:32.122820Z",
     "iopub.status.idle": "2024-10-31T13:35:32.141839Z",
     "shell.execute_reply": "2024-10-31T13:35:32.140613Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.123021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the horizon (12 months of 30 days each)\n",
    "fh = 30\n",
    "horizon = 12 * fh\n",
    "\n",
    "# Identify the unique dates in the dataset\n",
    "unique_dates = Y_df['ds'].unique()\n",
    "\n",
    "# Convert to a list and then sort the dates\n",
    "unique_dates = sorted(list(unique_dates))\n",
    "\n",
    "# Determine the cutoff date (cutoff at 12 months before the last date in the dataset)\n",
    "cutoff_date = unique_dates[-(horizon + 1)]\n",
    "\n",
    "# Training data: all data up to the cutoff date\n",
    "Y_train_df = Y_df[Y_df['ds'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e00fa-3d6f-4104-88db-a4993f9f5065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.143310Z",
     "iopub.status.busy": "2024-10-31T13:35:32.142880Z",
     "iopub.status.idle": "2024-10-31T13:35:32.202861Z",
     "shell.execute_reply": "2024-10-31T13:35:32.201770Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.143280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store the input and test sets\n",
    "input_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Loop to create the 6 input and test sets\n",
    "for i in range(6):\n",
    "    # Determine the start date of the test period\n",
    "    test_start_date = unique_dates[-(horizon - i * 2 * fh)]\n",
    "    test_end_date = unique_dates[-(horizon - (i * 2 * fh) - fh)]\n",
    "    \n",
    "    # Input data: all data up to the start of the current test period\n",
    "    input_df = Y_df[Y_df['ds'] <= test_start_date]\n",
    "    input_dfs.append(input_df)\n",
    "    \n",
    "    # Test data: the 30-day period following the start of the test period\n",
    "    test_df = Y_df[(Y_df['ds'] > test_start_date) & (Y_df['ds'] <= test_end_date)]\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "# Define the 6 input periods\n",
    "Y_input_df_0 = input_dfs[0]\n",
    "Y_input_df_1 = input_dfs[1]\n",
    "Y_input_df_2 = input_dfs[2]\n",
    "Y_input_df_3 = input_dfs[3]\n",
    "Y_input_df_4 = input_dfs[4]\n",
    "Y_input_df_5 = input_dfs[5]\n",
    "\n",
    "# Define the 6 test periods\n",
    "Y_test_df_0 = test_dfs[0]\n",
    "Y_test_df_1 = test_dfs[1]\n",
    "Y_test_df_2 = test_dfs[2]\n",
    "Y_test_df_3 = test_dfs[3]\n",
    "Y_test_df_4 = test_dfs[4]\n",
    "Y_test_df_5 = test_dfs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1df104-5b5e-4768-8bf7-a3dada2c8423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.204930Z",
     "iopub.status.busy": "2024-10-31T13:35:32.204524Z",
     "iopub.status.idle": "2024-10-31T13:35:32.709948Z",
     "shell.execute_reply": "2024-10-31T13:35:32.708917Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.204905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Timeserie to plot\n",
    "unique_id = '17'\n",
    "\n",
    "# Plot the train and test dataframes\n",
    "ts_utils.plot_train_test_split(Y_input_df_0, Y_test_df_0, unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b48977-f51a-431e-8ff6-245d0f37d9c8",
   "metadata": {},
   "source": [
    "## 4. Model Callibration\n",
    "\n",
    "### 4.1 Function to load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556b188-7460-4006-b1e4-1866d8d5011b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.711009Z",
     "iopub.status.busy": "2024-10-31T13:35:32.710784Z",
     "iopub.status.idle": "2024-10-31T13:35:32.719869Z",
     "shell.execute_reply": "2024-10-31T13:35:32.718841Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.710986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dataframe containing predictions\n",
    "def load_prediction_dataframe(model_name, period):\n",
    "    \n",
    "    # Retrieve the file path\n",
    "    path = f'predictions/{model_name}/insample/{period}/model_preds_2110_norm.csv'\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    # Check if we have a single file\n",
    "    if len(files) == 0:\n",
    "        print(f\"Error: No prediction files found for model {model_name} and period {period}\")\n",
    "        return None\n",
    "    elif len(files) > 1:\n",
    "        print(f\"Warning: Multiple prediction files found for model {model_name} and period {period}, using the first one.\")\n",
    "\n",
    "    # Retrieve the file name\n",
    "    filename = files[0]\n",
    "    print(f\"Loading prediction dataframe from {filename}\")\n",
    "\n",
    "    # Convert the csv file to a dataframe\n",
    "    df_pred = pd.read_csv(filename)\n",
    "    expected_columns = ['unique_id', 'ds', model_name]  # At minimum\n",
    "\n",
    "    # Check if we have the correct columns\n",
    "    if not all(col in df_pred.columns for col in expected_columns):\n",
    "        print(f\"Error: Prediction dataframe {filename} is missing expected columns.\")\n",
    "        print(\"The expected columns are: \", expected_columns)\n",
    "        return None\n",
    "\n",
    "    # Rename columns for 'ARIMA' and 'ETS' models\n",
    "    if model_name in ['ARIMA', 'ETS']:\n",
    "        col_mapping = {}\n",
    "        \n",
    "        for col in df_pred.columns:\n",
    "            if f'{model_name}-lo-' in col:\n",
    "                # 'ARIMA-lo-10' becomes 'ARIMA-lo-90'\n",
    "                num = col.split('-')[-1]\n",
    "                new_num = str(100 - int(num))\n",
    "                new_col = f'{model_name}-lo-{new_num}'\n",
    "                col_mapping[col] = new_col\n",
    "            # No changes needed for 'hi' columns\n",
    "        \n",
    "        if col_mapping:\n",
    "            df_pred = df_pred.rename(columns=col_mapping)\n",
    "            print(f\"Columns in {model_name} dataframe have been renamed to match expected format.\")\n",
    "\n",
    "    # Return the dataframe with the predictions\n",
    "    print(f\"Prediction dataframe {filename} loaded successfully with {len(df_pred)} rows.\")\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84207e9b-adef-4948-97aa-affd2d2df7e7",
   "metadata": {},
   "source": [
    "### 4.2 Function to calculate final horizon coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6baac-fde3-4619-8376-bc08223eb3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.721143Z",
     "iopub.status.busy": "2024-10-31T13:35:32.720807Z",
     "iopub.status.idle": "2024-10-31T13:35:32.738175Z",
     "shell.execute_reply": "2024-10-31T13:35:32.737004Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.721120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate coverage at a horizon\n",
    "def calculate_coverage_at_horizon(df_merged, model_name, horizon_days=30):\n",
    "    \n",
    "    # Ensure 'ds' is datetime\n",
    "    df_merged['ds'] = pd.to_datetime(df_merged['ds'])\n",
    "    \n",
    "    # Group by 'unique_id' to get min 'ds' for each\n",
    "    min_ds = df_merged.groupby('unique_id')['ds'].min().reset_index().rename(columns={'ds':'min_ds'})\n",
    "    \n",
    "    # Merge back to df_merged\n",
    "    df_merged = df_merged.merge(min_ds, on='unique_id')\n",
    "    \n",
    "    # Compute horizon as number of days since min_ds\n",
    "    df_merged['horizon'] = (df_merged['ds'] - df_merged['min_ds']).dt.days\n",
    "    \n",
    "    # Select rows where horizon == (horizon_days - 1)\n",
    "    target_horizon = horizon_days - 1  # since we start counting from 0\n",
    "    df_target = df_merged[df_merged['horizon'] == target_horizon].copy()\n",
    "    total_count = len(df_target)\n",
    "\n",
    "    # Check if we indeed have the horizon days\n",
    "    if total_count == 0:\n",
    "        print(f\"Warning: No data for horizon {horizon_days} days ahead.\")\n",
    "        return {}\n",
    "\n",
    "    # Initialize the confidence intervals\n",
    "    coverage_dict = {}\n",
    "    avg_relative_sizes = {}\n",
    "    median_relative_sizes = {}\n",
    "    ci_list = ['60%', '70%', '80%', '90%']\n",
    "\n",
    "    # Loop over the confidence intervals\n",
    "    for ci in ci_list:\n",
    "        # Get lower and upper bound column names\n",
    "        ci_num = ci.strip('%')\n",
    "        lo_col = f'{model_name}-lo-{ci_num}'\n",
    "        hi_col = f'{model_name}-hi-{ci_num}'\n",
    "\n",
    "        # Check if we have those columns\n",
    "        if lo_col not in df_target.columns or hi_col not in df_target.columns:\n",
    "            display(df_target)\n",
    "            print(f\"Warning: Columns {lo_col} or {hi_col} not found in dataframe.\")\n",
    "            continue\n",
    "    \n",
    "        # Calculate coverage\n",
    "        df_target[lo_col] = pd.to_numeric(df_target[lo_col], errors='coerce')\n",
    "        df_target[hi_col] = pd.to_numeric(df_target[hi_col], errors='coerce')\n",
    "        df_target['y'] = pd.to_numeric(df_target['y'], errors='coerce')\n",
    "        valid_rows = df_target.dropna(subset=['y', lo_col, hi_col])\n",
    "        valid_count = len(valid_rows)\n",
    "\n",
    "        # Check if we have valid rows for confidence interval\n",
    "        if valid_count == 0:\n",
    "            print(f\"Warning: No valid rows for confidence interval {ci} at horizon {horizon_days}\")\n",
    "            continue\n",
    "\n",
    "        # Check if it is in between the confidence interval\n",
    "        in_interval = ((valid_rows['y'] >= valid_rows[lo_col]) & (valid_rows['y'] <= valid_rows[hi_col])).astype(int)\n",
    "\n",
    "        # Calculate the percentage\n",
    "        count_in_interval = in_interval.sum()\n",
    "        coverage = count_in_interval / valid_count * 100\n",
    "        coverage_dict[ci] = coverage\n",
    "        \n",
    "        print(f\"For confidence interval {ci} at horizon {horizon_days} days, {count_in_interval}/{valid_count} ({coverage:.2f}%) of ground truth values are within the predicted interval.\")\n",
    "\n",
    "        # Calculate the interval size and relative interval size\n",
    "        interval_size = valid_rows[hi_col] - valid_rows[lo_col]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            relative_interval_size = interval_size / valid_rows['y'].abs()\n",
    "        \n",
    "        # Exclude cases where 'y' is zero or close to zero\n",
    "        relative_interval_size = relative_interval_size.replace([np.inf, -np.inf], np.nan)\n",
    "        relative_interval_size = relative_interval_size.dropna()\n",
    "    \n",
    "        # Compute average and median relative interval size\n",
    "        avg_relative_size = relative_interval_size.mean()\n",
    "        median_relative_size = relative_interval_size.median()\n",
    "    \n",
    "        # Store the results\n",
    "        avg_relative_sizes[ci] = avg_relative_size\n",
    "        median_relative_sizes[ci] = median_relative_size\n",
    "    \n",
    "        print(f\"Average relative interval size for {ci} confidence interval: {avg_relative_size:.4f}\")\n",
    "        print(f\"Median relative interval size for {ci} confidence interval: {median_relative_size:.4f}\")\n",
    "    \n",
    "    return coverage_dict, avg_relative_sizes, median_relative_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b010093-903b-495a-9922-75e4f74b7a11",
   "metadata": {},
   "source": [
    "### 4.3 Retrieve model callibration numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24247a-bdd3-45a4-8682-baa5f4bb627c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.739228Z",
     "iopub.status.busy": "2024-10-31T13:35:32.738936Z",
     "iopub.status.idle": "2024-10-31T13:35:32.748726Z",
     "shell.execute_reply": "2024-10-31T13:35:32.747960Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.739205Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of models and periods\n",
    "models = ['ARIMA', 'ETS', 'PatchTST', 'DeepAR', 'TimesNet', 'NHITS', 'Chronos-large', 'Chronos-small', 'Chronos-FT']\n",
    "periods = ['period01', 'period02', 'period03', 'period04', 'period05', 'period06']\n",
    "horizon_days = 30\n",
    "\n",
    "# Map periods to existing test dataframes\n",
    "period_to_test_df = {\n",
    "    'period01': Y_test_df_0,\n",
    "    'period02': Y_test_df_1,\n",
    "    'period03': Y_test_df_2,\n",
    "    'period04': Y_test_df_3,\n",
    "    'period05': Y_test_df_4,\n",
    "    'period06': Y_test_df_5\n",
    "}\n",
    "\n",
    "# Initialize results dataframe\n",
    "index = pd.MultiIndex.from_product([periods, ['60%', '70%', '80%', '90%']], names=['Period', 'Confidence Interval'])\n",
    "results_df = pd.DataFrame(index=index, columns=models)\n",
    "\n",
    "# Initialize dataframes to store average and median relative interval sizes\n",
    "avg_size_df = pd.DataFrame(index=index, columns=models)\n",
    "median_size_df = pd.DataFrame(index=index, columns=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c887b6-7a0e-40c2-b443-68e42cab4073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:32.750560Z",
     "iopub.status.busy": "2024-10-31T13:35:32.750266Z",
     "iopub.status.idle": "2024-10-31T13:35:35.185672Z",
     "shell.execute_reply": "2024-10-31T13:35:35.184560Z",
     "shell.execute_reply.started": "2024-10-31T13:35:32.750537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over all the periods\n",
    "for period in periods:\n",
    "    print(f\"\\nProcessing period: {period}\")\n",
    "    \n",
    "    # Get the test dataframe for the current period\n",
    "    df_test = period_to_test_df.get(period)\n",
    "\n",
    "    # Check if we have a test dataframe\n",
    "    if df_test is None:\n",
    "        print(f\"Error: Test dataframe for {period} not found.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Test dataframe for {period} loaded successfully with {len(df_test)} rows.\")\n",
    "        \n",
    "        # Ensure required columns are present\n",
    "        expected_columns = ['unique_id', 'ds', 'y']\n",
    "        \n",
    "        if not all(col in df_test.columns for col in expected_columns):\n",
    "            print(f\"Error: Test dataframe for {period} is missing required columns.\")\n",
    "            continue\n",
    "\n",
    "    # Loop over all the models\n",
    "    for model_name in models:\n",
    "        print(f\"\\nProcessing model: {model_name} for period: {period}\")\n",
    "\n",
    "        # Load the prediction dataframe\n",
    "        df_pred = load_prediction_dataframe(model_name, period)\n",
    "\n",
    "        # Check if the prediction dataframe is present\n",
    "        if df_pred is None:\n",
    "            continue\n",
    "\n",
    "        # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "        df_pred['unique_id'] = df_pred['unique_id'].astype('string')\n",
    "        df_pred['ds'] = pd.to_datetime(df_pred['ds'])\n",
    "        \n",
    "        # Merge df_test and df_pred on 'unique_id' and 'ds'\n",
    "        df_merged = pd.merge(df_test, df_pred, on=['unique_id', 'ds'], how='inner')\n",
    "        print(f\"Merged dataframe has {len(df_merged)} rows.\")\n",
    "\n",
    "        # Check if the merged dataframe is not empty\n",
    "        if len(df_merged) == 0:\n",
    "            print(f\"Error: Merged dataframe is empty for model {model_name} and period {period}.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate coverage at specified horizon\n",
    "        coverage_dict, avg_relative_sizes, median_relative_sizes = calculate_coverage_at_horizon(df_merged, model_name, horizon_days=horizon_days)\n",
    "        \n",
    "        # Fill in coverage results\n",
    "        for ci, coverage in coverage_dict.items():\n",
    "            results_df.loc[(period, ci), model_name] = coverage\n",
    "        \n",
    "        # Fill in average relative interval sizes\n",
    "        for ci, avg_size in avg_relative_sizes.items():\n",
    "            avg_size_df.loc[(period, ci), model_name] = avg_size\n",
    "        \n",
    "        # Fill in median relative interval sizes\n",
    "        for ci, median_size in median_relative_sizes.items():\n",
    "            median_size_df.loc[(period, ci), model_name] = median_size\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "print(results_df)\n",
    "# Save results_df to CSV if needed\n",
    "results_df.to_csv('coverage_results_at_horizon.csv')\n",
    "print(\"\\nResults saved to coverage_results_at_horizon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fdefb9-5d55-4723-b52e-2dcb758fe243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:35.243770Z",
     "iopub.status.busy": "2024-10-31T13:35:35.243486Z",
     "iopub.status.idle": "2024-10-31T13:35:35.274278Z",
     "shell.execute_reply": "2024-10-31T13:35:35.273431Z",
     "shell.execute_reply.started": "2024-10-31T13:35:35.243746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean over the periods for each confidence interval and model\n",
    "mean_df = results_df.groupby(level='Confidence Interval').mean()\n",
    "\n",
    "# Calculate the standard deviation over the periods for each confidence interval and model\n",
    "std_df = results_df.groupby(level='Confidence Interval').std()\n",
    "\n",
    "# Display the mean dataframe\n",
    "print(\"Mean coverage over periods:\")\n",
    "display(mean_df)\n",
    "\n",
    "# Display the standard deviation dataframe\n",
    "print(\"\\nStandard deviation of coverage over periods:\")\n",
    "display(std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051333ee-7a96-4df4-9c76-7a2c09e52412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:35.275448Z",
     "iopub.status.busy": "2024-10-31T13:35:35.275223Z",
     "iopub.status.idle": "2024-10-31T13:35:35.287070Z",
     "shell.execute_reply": "2024-10-31T13:35:35.285916Z",
     "shell.execute_reply.started": "2024-10-31T13:35:35.275427Z"
    }
   },
   "outputs": [],
   "source": [
    "def Plotting_Coverage(mean_df, std_df, model_categories, colors):\n",
    "    # Extract the list of confidence intervals\n",
    "    confidence_intervals = mean_df.index.tolist()  # ['60%', '70%', '80%', '90%']\n",
    "\n",
    "    # Set up 2x2 grid of subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
    "    axes = axes.flatten()  # Flatten to 1D array for easy iteration\n",
    "\n",
    "    # Error bar config\n",
    "    error_config = {'ecolor': 'black', 'elinewidth': 1, 'capsize': 3}\n",
    "\n",
    "    # Loop over confidence intervals and corresponding axes\n",
    "    for ax, ci in zip(axes, confidence_intervals):\n",
    "        # Extract data for the current confidence interval\n",
    "        mean_data = mean_df.loc[ci]\n",
    "        std_data = std_df.loc[ci]\n",
    "\n",
    "        # Sort models by mean coverage (ascending)\n",
    "        sorted_mean_data = mean_data.sort_values(ascending=True)\n",
    "        sorted_std_data = std_data[sorted_mean_data.index]\n",
    "\n",
    "        models = sorted_mean_data.index.tolist()\n",
    "        mean_values = sorted_mean_data.values\n",
    "        std_values = sorted_std_data.values\n",
    "\n",
    "        # Get colors for the models based on their categories\n",
    "        colors_list = [colors[model_categories[model]] for model in models]\n",
    "\n",
    "        # Plot horizontal bars with error bars\n",
    "        bars = ax.barh(models, mean_values, xerr=std_values, color=colors_list,\n",
    "                       error_kw=error_config)\n",
    "\n",
    "        # Set labels and invert y-axis to have highest value at top\n",
    "        ax.set_xlabel('Percentage (%) of Ground Truth within Confidence Interval', fontsize=11)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        if ax in axes[:2]:\n",
    "            ax.set_xlabel('')\n",
    "\n",
    "        # Set title to include the confidence interval\n",
    "        ax.set_title(f'Callibration at {ci} Confidence Interval', fontsize=14, pad=15)\n",
    "\n",
    "        # Add dashed line at the true confidence level\n",
    "        true_confidence = float(ci.strip('%'))\n",
    "        ax.axvline(x=true_confidence, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Annotate bars with coverage values\n",
    "        for i, (v, std) in enumerate(zip(mean_values, std_values)):\n",
    "            ax.text(v + std + 0.5, i, f'{v:.2f}%', \n",
    "                    color='black', va='center', fontsize=10)\n",
    "\n",
    "        # Set x-axis limits to accommodate error bars and annotations\n",
    "        max_x = max(mean_values + std_values)\n",
    "        ax.set_xlim(0, max_x + 10)\n",
    "\n",
    "    # Prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95], h_pad=2.0)\n",
    "\n",
    "    # Legend for model categories\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color) for color in colors.values()]\n",
    "    labels = colors.keys()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(labels), fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018beed8-0a0e-414d-979a-a111e6d6682f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:35.288066Z",
     "iopub.status.busy": "2024-10-31T13:35:35.287822Z",
     "iopub.status.idle": "2024-10-31T13:35:35.294384Z",
     "shell.execute_reply": "2024-10-31T13:35:35.293075Z",
     "shell.execute_reply.started": "2024-10-31T13:35:35.288041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model categories\n",
    "model_categories = {\n",
    "    'Naive': 'Baseline Model',\n",
    "    'ARIMA': 'Baseline Model',\n",
    "    'ETS': 'Baseline Model',\n",
    "    'ES_bu': 'Baseline Model',\n",
    "    'PatchTST': 'Deep Model',\n",
    "    'NHITS': 'Deep Model',\n",
    "    'DeepAR': 'Deep Model',\n",
    "    'TimesNet': 'Deep Model',\n",
    "    'ESRNN': 'Deep Model',\n",
    "    'Chronos (small)': 'Pre-trained Model',\n",
    "    'Chronos (large)': 'Pre-trained Model',\n",
    "    'Chronos (FT)': 'Pre-trained Model',\n",
    "    'Chronos': 'Pre-trained Model',\n",
    "    'TimesFM': 'Pre-trained Model',\n",
    "    'TimesFM (FT)': 'Pre-trained Model',\n",
    "    'Chronos-small': 'Pre-trained Model',\n",
    "    'Chronos-large': 'Pre-trained Model'\n",
    "}\n",
    "\n",
    "# Define lighter colors for each category\n",
    "colors = {\n",
    "    'Deep Model': '#FFA07A',\n",
    "    'Pre-trained Model': '#9370DB',\n",
    "    'Baseline Model': '#ADD8E6' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b7b5-28f8-47b5-ac8c-06f08a6f4487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:35.295327Z",
     "iopub.status.busy": "2024-10-31T13:35:35.295127Z",
     "iopub.status.idle": "2024-10-31T13:35:36.049067Z",
     "shell.execute_reply": "2024-10-31T13:35:36.048017Z",
     "shell.execute_reply.started": "2024-10-31T13:35:35.295306Z"
    }
   },
   "outputs": [],
   "source": [
    "Plotting_Coverage(mean_df, std_df, model_categories, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4878bf1",
   "metadata": {},
   "source": [
    "### Confidence Interval Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c80559-b89a-4fe4-a8b5-94b83be067da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:36.050054Z",
     "iopub.status.busy": "2024-10-31T13:35:36.049838Z",
     "iopub.status.idle": "2024-10-31T13:35:36.090896Z",
     "shell.execute_reply": "2024-10-31T13:35:36.090218Z",
     "shell.execute_reply.started": "2024-10-31T13:35:36.050031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean over the periods for each confidence interval and model\n",
    "mean_median_size_df = median_size_df.groupby(level='Confidence Interval').mean()\n",
    "\n",
    "# Calculate the standard deviation over the periods for each confidence interval and model\n",
    "std_median_size_df = median_size_df.groupby(level='Confidence Interval').std()\n",
    "\n",
    "# Display the mean dataframe\n",
    "print(\"Mean coverage over periods:\")\n",
    "display(mean_median_size_df)\n",
    "\n",
    "# Display the standard deviation dataframe\n",
    "print(\"\\nStandard deviation of coverage over periods:\")\n",
    "display(std_median_size_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c3f6c-b7ba-4d21-b812-105b2d83a883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:36.092051Z",
     "iopub.status.busy": "2024-10-31T13:35:36.091741Z",
     "iopub.status.idle": "2024-10-31T13:35:36.101044Z",
     "shell.execute_reply": "2024-10-31T13:35:36.100371Z",
     "shell.execute_reply.started": "2024-10-31T13:35:36.092025Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotting_ci_size(mean_df, std_df, model_categories, colors, confidence_interval):\n",
    "    # Is the confidence interval is valid\n",
    "    if confidence_interval not in mean_df.index.tolist():\n",
    "        raise ValueError(f\"Confidence interval must be one of {mean_df.index.tolist()}\")\n",
    "\n",
    "    # Create a single figure\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "    # Error bar configuration\n",
    "    error_config = {'ecolor': 'black', 'elinewidth': 1, 'capsize': 3}\n",
    "\n",
    "    # Extract data for the specified confidence interval\n",
    "    mean_data = mean_df.loc[confidence_interval]\n",
    "    std_data = std_df.loc[confidence_interval]\n",
    "\n",
    "    # Sort models by mean coverage (ascending)\n",
    "    sorted_mean_data = mean_data.sort_values(ascending=True)\n",
    "    sorted_std_data = std_data[sorted_mean_data.index]\n",
    "\n",
    "    models = sorted_mean_data.index.tolist()\n",
    "    mean_values = sorted_mean_data.values\n",
    "    std_values = sorted_std_data.values\n",
    "\n",
    "    # Get colors for the models based on their categories\n",
    "    colors_list = [colors[model_categories[model]] for model in models]\n",
    "\n",
    "    # Plot horizontal bars with error bars\n",
    "    bars = ax.barh(models, mean_values, xerr=std_values, color=colors_list,\n",
    "                   error_kw=error_config)\n",
    "\n",
    "    # Set labels and invert y-axis to have highest value at top\n",
    "    ax.set_xlabel('Confidence Interval Width Relative to Ground Truth (%)', fontsize=10)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Set title to include the confidence interval\n",
    "    ax.set_title(f'Confidence Interval Sizes at {confidence_interval} Confidence Interval', fontsize=13, pad=15)\n",
    "\n",
    "    # Annotate bars with coverage values\n",
    "    for i, (v, std) in enumerate(zip(mean_values, std_values)):\n",
    "        ax.text(v + std + 0.05, i, f'{v:.2f}%', \n",
    "                color='black', va='center', fontsize=8)\n",
    "\n",
    "    # Calculate better x-axis limits\n",
    "    max_value_with_error = max(mean_values + std_values)\n",
    "\n",
    "    # Add 2% padding for annotations and readability\n",
    "    x_max = min(max_value_with_error * 1.2, max_value_with_error + 2)\n",
    "    ax.set_xlim(0, x_max)\n",
    "\n",
    "    # Place legend inside the plot in the top right\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color) for color in colors.values()]\n",
    "    labels = colors.keys()\n",
    "    ax.legend(handles, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eff31e-ea1f-424c-aebe-6a05dfce0e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:35:36.102110Z",
     "iopub.status.busy": "2024-10-31T13:35:36.101677Z",
     "iopub.status.idle": "2024-10-31T13:35:36.286944Z",
     "shell.execute_reply": "2024-10-31T13:35:36.285962Z",
     "shell.execute_reply.started": "2024-10-31T13:35:36.102088Z"
    }
   },
   "outputs": [],
   "source": [
    "plotting_ci_size(mean_median_size_df, std_median_size_df, model_categories, colors, confidence_interval='90%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e0f0e-2aef-4cfa-a5e2-9a0d85506de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b43b4-32d7-4961-830b-10a74b80e57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAP Environment",
   "language": "python",
   "name": "dap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
