{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae0b593-0b44-442d-bb29-b35b5283b0e3",
   "metadata": {},
   "source": [
    "# Timeseries Forecasting on Transaction Data\n",
    "\n",
    "## 1. Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7893b-71a9-443c-ae5b-8606851c5849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:20:58.780006Z",
     "iopub.status.busy": "2024-10-30T15:20:58.779164Z",
     "iopub.status.idle": "2024-10-30T15:20:58.785866Z",
     "shell.execute_reply": "2024-10-30T15:20:58.784474Z",
     "shell.execute_reply.started": "2024-10-30T15:20:58.779940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Pip for evaluation metrics\n",
    "# !pip install datasetsforecast\n",
    "# !pip install sktime\n",
    "# !pip install EntropyHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12442afe-f1e1-494a-b261-828b837d04e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:20:58.788336Z",
     "iopub.status.busy": "2024-10-30T15:20:58.787789Z",
     "iopub.status.idle": "2024-10-30T15:21:01.369048Z",
     "shell.execute_reply": "2024-10-30T15:21:01.367964Z",
     "shell.execute_reply.started": "2024-10-30T15:20:58.788293Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Some functions for plotting and stuff\n",
    "import ts_utils as ts_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ff974-b634-413d-813b-447c8fec8dce",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e7320-d2ec-45ed-8091-79ca332a368d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:01.370888Z",
     "iopub.status.busy": "2024-10-30T15:21:01.370305Z",
     "iopub.status.idle": "2024-10-30T15:21:01.457787Z",
     "shell.execute_reply": "2024-10-30T15:21:01.456680Z",
     "shell.execute_reply.started": "2024-10-30T15:21:01.370863Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Size of the data to read\n",
    "data_size = 'full'\n",
    "\n",
    "# Date of the data to read\n",
    "data_date = '2110' # '2110' = 21st of October\n",
    "\n",
    "# Read the data (takes around 2 minutes)\n",
    "dataset = pd.read_csv(f\"~/Thesis/data/eod_balances_{data_date}_{data_size}.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34465b37-99fa-453d-9ce3-5814e0e24caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:56:49.027930Z",
     "iopub.status.busy": "2024-07-02T11:56:49.026975Z",
     "iopub.status.idle": "2024-07-02T11:56:49.032924Z",
     "shell.execute_reply": "2024-07-02T11:56:49.031885Z",
     "shell.execute_reply.started": "2024-07-02T11:56:49.027866Z"
    },
    "tags": []
   },
   "source": [
    "### 2.1 In-sample and Out-sample split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370ae60-b7e3-4518-9adf-21c367743104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:01.458542Z",
     "iopub.status.busy": "2024-10-30T15:21:01.458319Z",
     "iopub.status.idle": "2024-10-30T15:21:01.465837Z",
     "shell.execute_reply": "2024-10-30T15:21:01.464679Z",
     "shell.execute_reply.started": "2024-10-30T15:21:01.458520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate total amount of timeseries\n",
    "num_timeseries = len(dataset.columns) - 1\n",
    "\n",
    "# Specify train test split percentage\n",
    "train_test_split = 0.8\n",
    "\n",
    "# Split into train and out of sample test data\n",
    "num_out_of_sample = int(train_test_split * num_timeseries)\n",
    "\n",
    "# Create in-sample dataframe\n",
    "in_sample_data = dataset.iloc[:, : num_out_of_sample + 1] # Training and testing\n",
    "\n",
    "# Create out-sample dataframe\n",
    "n = num_timeseries-num_out_of_sample\n",
    "columns_to_keep = dataset.columns[[0]].tolist() + dataset.columns[-n:].tolist()\n",
    "out_sample_data = dataset[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1d7dc-20f8-44dc-9c2b-333f2043da10",
   "metadata": {},
   "source": [
    "## 3. In-sample analysis\n",
    "\n",
    "### 3.1 Train/Test splitting and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbff281-2233-4418-b2a4-cde4357d67c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:01.467787Z",
     "iopub.status.busy": "2024-10-30T15:21:01.467575Z",
     "iopub.status.idle": "2024-10-30T15:21:01.537542Z",
     "shell.execute_reply": "2024-10-30T15:21:01.536730Z",
     "shell.execute_reply.started": "2024-10-30T15:21:01.467766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the data to the long format\n",
    "Y_df = in_sample_data.melt(id_vars=['date'], var_name='unique_id', value_name='y')\n",
    "Y_df = Y_df.rename(columns={'date':'ds'})\n",
    "\n",
    "# Convert date column to datetime type\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de5070-2402-4713-a7d5-3437145a913c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:01.538621Z",
     "iopub.status.busy": "2024-10-30T15:21:01.538350Z",
     "iopub.status.idle": "2024-10-30T15:21:01.552957Z",
     "shell.execute_reply": "2024-10-30T15:21:01.551907Z",
     "shell.execute_reply.started": "2024-10-30T15:21:01.538598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the horizon (12 months of 30 days each)\n",
    "fh = 30\n",
    "horizon = 12 * fh\n",
    "\n",
    "# Identify the unique dates in the dataset\n",
    "unique_dates = Y_df['ds'].unique()\n",
    "\n",
    "# Convert to a list and then sort the dates\n",
    "unique_dates = sorted(list(unique_dates))\n",
    "\n",
    "# Determine the cutoff date (cutoff at 12 months before the last date in the dataset)\n",
    "cutoff_date = unique_dates[-(horizon + 1)]\n",
    "\n",
    "# Training data: all data up to the cutoff date\n",
    "Y_train_df = Y_df[Y_df['ds'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14a7f0-fc32-4bb9-834c-fcf45df0d6df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:01.554115Z",
     "iopub.status.busy": "2024-10-30T15:21:01.553887Z",
     "iopub.status.idle": "2024-10-30T15:21:01.642400Z",
     "shell.execute_reply": "2024-10-30T15:21:01.641650Z",
     "shell.execute_reply.started": "2024-10-30T15:21:01.554092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store the input and test sets\n",
    "input_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Loop to create the 6 input and test sets\n",
    "for i in range(6):\n",
    "    # Determine the start date of the test period\n",
    "    test_start_date = unique_dates[-(horizon - i * 2 * fh)]\n",
    "    test_end_date = unique_dates[-(horizon - (i * 2 * fh) - fh)]\n",
    "    \n",
    "    # Input data: all data up to the start of the current test period\n",
    "    input_df = Y_df[Y_df['ds'] <= test_start_date]\n",
    "    input_dfs.append(input_df)\n",
    "    \n",
    "    # Test data: the 30-day period following the start of the test period\n",
    "    test_df = Y_df[(Y_df['ds'] > test_start_date) & (Y_df['ds'] <= test_end_date)]\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "# Define the 6 input periods\n",
    "Y_input_df_0 = input_dfs[0]\n",
    "Y_input_df_1 = input_dfs[1]\n",
    "Y_input_df_2 = input_dfs[2]\n",
    "Y_input_df_3 = input_dfs[3]\n",
    "Y_input_df_4 = input_dfs[4]\n",
    "Y_input_df_5 = input_dfs[5]\n",
    "\n",
    "# Define the 6 test periods\n",
    "Y_test_df_0 = test_dfs[0]\n",
    "Y_test_df_1 = test_dfs[1]\n",
    "Y_test_df_2 = test_dfs[2]\n",
    "Y_test_df_3 = test_dfs[3]\n",
    "Y_test_df_4 = test_dfs[4]\n",
    "Y_test_df_5 = test_dfs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0e8b2-55e4-42b0-9d29-021f1b96d4b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:01.643354Z",
     "iopub.status.busy": "2024-10-30T15:21:01.643124Z",
     "iopub.status.idle": "2024-10-30T15:21:02.191363Z",
     "shell.execute_reply": "2024-10-30T15:21:02.190360Z",
     "shell.execute_reply.started": "2024-10-30T15:21:01.643332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Timeserie to plot\n",
    "unique_id = '17'\n",
    "\n",
    "# Plot the train and test dataframes\n",
    "ts_utils.plot_train_test_split(Y_input_df_0, Y_test_df_0, unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dad8a6-5d1f-4d54-8052-2c9378847885",
   "metadata": {},
   "source": [
    "### 3.2 Retrieve Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30751828-1a25-43fd-9932-588bd461ca05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:02.192384Z",
     "iopub.status.busy": "2024-10-30T15:21:02.192170Z",
     "iopub.status.idle": "2024-10-30T15:21:02.198171Z",
     "shell.execute_reply": "2024-10-30T15:21:02.196911Z",
     "shell.execute_reply.started": "2024-10-30T15:21:02.192362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the models and periods\n",
    "models = ['Naive', 'ARIMA', 'ETS', 'NHITS', 'PatchTST', 'TimesNet', 'DeepAR', 'Chronos-small', 'Chronos-large', 'Chronos-FT']\n",
    "periods = ['period01', 'period02', 'period03', 'period04', 'period05', 'period06']\n",
    "\n",
    "# Create Y_test_dfs as a list of test dataframes for each period\n",
    "Y_test_dfs = [Y_test_df_0, Y_test_df_1, Y_test_df_2, Y_test_df_3, Y_test_df_4, Y_test_df_5]\n",
    "\n",
    "# Initialize a dictionary to hold the prediction dataframes for each period\n",
    "Y_pred_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32f288-fb9b-4ded-83b4-5d4e24024e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:02.200361Z",
     "iopub.status.busy": "2024-10-30T15:21:02.199717Z",
     "iopub.status.idle": "2024-10-30T15:21:02.208980Z",
     "shell.execute_reply": "2024-10-30T15:21:02.207623Z",
     "shell.execute_reply.started": "2024-10-30T15:21:02.200303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjusted merging_preds function\n",
    "def merging_preds(Y_pred_df, model_preds, model_name):\n",
    "    # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "    model_preds['unique_id'] = model_preds['unique_id'].astype('string')\n",
    "    model_preds['ds'] = pd.to_datetime(model_preds['ds'])\n",
    "    \n",
    "    # Merge predictions on 'unique_id' and 'ds'\n",
    "    Y_pred_df = Y_pred_df.merge(model_preds[['unique_id', 'ds', f'{model_name}']], on=['unique_id', 'ds'], how='left')\n",
    "    \n",
    "    return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a68f0-0f1e-4cff-8594-aaa4a9f10b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:02.211918Z",
     "iopub.status.busy": "2024-10-30T15:21:02.211468Z",
     "iopub.status.idle": "2024-10-30T15:21:04.442201Z",
     "shell.execute_reply": "2024-10-30T15:21:04.441448Z",
     "shell.execute_reply.started": "2024-10-30T15:21:02.211886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over periods to get predictions\n",
    "for i, period in enumerate(periods):\n",
    "    print(f\"Processing {period}...\")\n",
    "    \n",
    "    # Get the test dataframe for this period\n",
    "    Y_pred_df = Y_test_dfs[i].copy()\n",
    "    \n",
    "    # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "    Y_pred_df['unique_id'] = Y_pred_df['unique_id'].astype('string')\n",
    "    Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "    \n",
    "    # Loop over models to merge predictions\n",
    "    for model in models:\n",
    "        # Read the prediction csv\n",
    "        model_preds = pd.read_csv(f\"predictions/{model}/insample/{period}/model_preds_{data_date}_{data_size}.csv\")\n",
    "        \n",
    "        # Merge the predictions into Y_pred_df\n",
    "        Y_pred_df = merging_preds(Y_pred_df, model_preds, model)\n",
    "\n",
    "    # Set 'unique_id' as index if needed\n",
    "    Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "    \n",
    "    # Rename columns if necessary\n",
    "    Y_pred_df = Y_pred_df.rename(columns={\"Chronos-small\": \"Chronos (small)\", \"Chronos-large\": \"Chronos (large)\"})\n",
    "    \n",
    "    # Store the dataframe in the dictionary\n",
    "    Y_pred_dfs[period] = Y_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49498292-735d-4a13-ad47-3a2915790017",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "### 4.1 Visually plot forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8204f74-2bac-4a96-bba4-4eafb2290913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:04.443182Z",
     "iopub.status.busy": "2024-10-30T15:21:04.442961Z",
     "iopub.status.idle": "2024-10-30T15:21:05.845971Z",
     "shell.execute_reply": "2024-10-30T15:21:05.844874Z",
     "shell.execute_reply.started": "2024-10-30T15:21:04.443160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify models to plot\n",
    "model_names = ['ARIMA', 'PatchTST', 'NHITS', 'Chronos (large)']\n",
    "\n",
    "# Specify accounts to plot\n",
    "unique_ids = ['1', '2', '3', '4', '5']\n",
    "\n",
    "# Plot the predictions\n",
    "ts_utils.plot_multiple_model_forecasts(Y_train_df, Y_test_df_0, Y_pred_dfs['period01'].reset_index(), model_names, unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788367d-2ed9-429a-b8f6-fe8db523656d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:05.849398Z",
     "iopub.status.busy": "2024-10-30T15:21:05.848726Z",
     "iopub.status.idle": "2024-10-30T15:21:06.728858Z",
     "shell.execute_reply": "2024-10-30T15:21:06.728150Z",
     "shell.execute_reply.started": "2024-10-30T15:21:05.849347Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify models to plot\n",
    "model_names = ['ARIMA', 'PatchTST', 'Chronos-small', 'Chronos-FT']\n",
    "\n",
    "# Specify accounts to plot\n",
    "unique_ids = [str(i) for i in np.random.randint(0, 20, size=3)]\n",
    "\n",
    "# Plot the predictions\n",
    "ts_utils.plot_multiple_model_forecasts(Y_train_df, Y_test_df_0, Y_pred_dfs['period01'].reset_index(), model_names, unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c752773-a097-4ebc-9dbe-ed301e56071a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:33:54.832890Z",
     "iopub.status.busy": "2024-10-30T15:33:54.832234Z",
     "iopub.status.idle": "2024-10-30T15:33:56.521052Z",
     "shell.execute_reply": "2024-10-30T15:33:56.520193Z",
     "shell.execute_reply.started": "2024-10-30T15:33:54.832844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify models to plot\n",
    "model_names = ['Chronos (large)']\n",
    "\n",
    "# Specify accounts to plot\n",
    "unique_ids = ['28', '27', '26', '25', '24', '23']\n",
    "\n",
    "# Plot the predictions\n",
    "ts_utils.plot_multiple_model_forecasts(Y_train_df, Y_test_df_0, Y_pred_dfs['period01'].reset_index(), model_names, unique_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80904d9b-e454-4ca2-a7b9-9a4da77d66b8",
   "metadata": {},
   "source": [
    "### 4.1 Retrieve the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fed32b-c44c-4480-a2f6-eb4b5ece5d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:06.729841Z",
     "iopub.status.busy": "2024-10-30T15:21:06.729618Z",
     "iopub.status.idle": "2024-10-30T15:21:07.028985Z",
     "shell.execute_reply": "2024-10-30T15:21:07.027443Z",
     "shell.execute_reply.started": "2024-10-30T15:21:06.729819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define periods and horizons\n",
    "periods = [f'period{i+1:02d}' for i in range(6)]  # ['period01', 'period02', ..., 'period06']\n",
    "horizons = ['1_day', '7_day', '14_day', '30_day']\n",
    "\n",
    "# Initialize a nested dictionary to store dataframes\n",
    "metrics = {}\n",
    "\n",
    "# Loop over periods and horizons to read the dataframes\n",
    "for period_name in periods:\n",
    "    metrics[period_name] = {}\n",
    "    for horizon in horizons:\n",
    "        # Construct the filename\n",
    "        filename = f\"metrics/insample/{period_name}/metrics_{horizon}_{data_date}_{data_size}.csv\"\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        # Ensure 'unique_id' is of type string\n",
    "        df['unique_id'] = df['unique_id'].astype('string')\n",
    "        # Store the dataframe\n",
    "        metrics[period_name][horizon] = df\n",
    "\n",
    "# Initialize a dictionary to store the summarized metrics for each period\n",
    "metric_results = {}\n",
    "\n",
    "# Loop over each period to summarize the metrics\n",
    "for period_name in periods:\n",
    "    # Retrieve the four dataframes for the current period\n",
    "    df1 = metrics[period_name]['1_day']\n",
    "    df2 = metrics[period_name]['7_day']\n",
    "    df3 = metrics[period_name]['14_day']\n",
    "    df4 = metrics[period_name]['30_day']\n",
    "    \n",
    "    # Summarize the metrics using ts_utils.summarize_metrics()\n",
    "    result = ts_utils.summarize_metrics(df1, df2, df3, df4)\n",
    "    \n",
    "    # Store the result\n",
    "    metric_results[period_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5c29b-f373-436e-870c-137074e4913b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.030776Z",
     "iopub.status.busy": "2024-10-30T15:21:07.030185Z",
     "iopub.status.idle": "2024-10-30T15:21:07.054381Z",
     "shell.execute_reply": "2024-10-30T15:21:07.053344Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.030736Z"
    }
   },
   "outputs": [],
   "source": [
    "# metric_results['period01'], metric_results['period02'], ..., metric_results['period06']\n",
    "metric_results['period02']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24f995-430e-4e6f-ad10-bff637590f1f",
   "metadata": {},
   "source": [
    "### 4.1.2 Filtering the MAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7defc-4dc7-4cdc-b6e8-983faeac3e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.055506Z",
     "iopub.status.busy": "2024-10-30T15:21:07.055270Z",
     "iopub.status.idle": "2024-10-30T15:21:07.064984Z",
     "shell.execute_reply": "2024-10-30T15:21:07.063797Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.055485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the filter_high_mape function with detailed print statements\n",
    "def filter_high_mape(eval_df, Y_test_df, mape_threshold=500, approx_0_threshold=100, debug=False):\n",
    "    # Step 1: Identify unique_ids where MAPE exceeds the threshold\n",
    "    exceeding_error_ids = eval_df[\n",
    "        (eval_df['metric'] == 'mape') & \n",
    "        (eval_df['PatchTST'] > mape_threshold)\n",
    "    ]['unique_id'].unique()\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"    Unique IDs with MAPE > {mape_threshold}: {len(exceeding_error_ids)}\")\n",
    "        print(f\"    Unique IDs exceeding MAPE threshold: {list(exceeding_error_ids)}\")\n",
    "\n",
    "    # Step 2: Identify unique_ids where y is close to zero\n",
    "    filtered_ids = Y_test_df[\n",
    "        (Y_test_df['y'] >= -approx_0_threshold) & \n",
    "        (Y_test_df['y'] <= approx_0_threshold)\n",
    "    ]['unique_id'].unique()\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"    Unique IDs with y close to zero (|y| <= {approx_0_threshold}): {len(filtered_ids)}\")\n",
    "        print(f\"    Unique IDs with y close to zero: {list(filtered_ids)}\")\n",
    "\n",
    "    # Step 3: Find intersection of IDs to remove\n",
    "    final_unique_ids = set(exceeding_error_ids).intersection(set(filtered_ids))\n",
    "\n",
    "    if debug:\n",
    "        print(f\"    Unique IDs to remove (intersection): {len(final_unique_ids)}\")\n",
    "        print(f\"    Unique IDs to remove: {list(final_unique_ids)}\")\n",
    "\n",
    "    # Step 4: Remove these unique_ids from eval_df\n",
    "    filtered_eval_df = eval_df[~eval_df['unique_id'].isin(final_unique_ids)]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"    Filtered eval_df has {len(filtered_eval_df)} rows (original had {len(eval_df)} rows)\")\n",
    "\n",
    "    return filtered_eval_df\n",
    "\n",
    "# Define periods and horizons\n",
    "periods = [f'period{i+1:02d}' for i in range(6)]  # ['period01', 'period02', ..., 'period06']\n",
    "horizons = ['1_day', '7_day', '14_day', '30_day']\n",
    "\n",
    "# Initialize nested dictionaries to store original and filtered dataframes\n",
    "metrics = {}\n",
    "filtered_metrics = {}\n",
    "\n",
    "# Create a mapping of period names to their corresponding Y_test_df\n",
    "Y_test_dfs = {f'period{i+1:02d}': test_dfs[i] for i in range(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827cfff-376a-468a-a85b-fa8e33be0de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.065981Z",
     "iopub.status.busy": "2024-10-30T15:21:07.065772Z",
     "iopub.status.idle": "2024-10-30T15:21:07.221797Z",
     "shell.execute_reply": "2024-10-30T15:21:07.220528Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.065960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over periods and horizons to read the dataframes and apply filtering\n",
    "for period_name in periods:\n",
    "    print(f\"Processing {period_name}...\")\n",
    "    metrics[period_name] = {}\n",
    "    filtered_metrics[period_name] = {}\n",
    "    \n",
    "    # Get the corresponding Y_test_df for the current period\n",
    "    Y_test_df = Y_test_dfs[period_name]\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        # Construct the filename\n",
    "        filename = f\"metrics/insample/{period_name}/metrics_{horizon}_{data_date}_{data_size}.csv\"\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Ensure 'unique_id' is of type string\n",
    "        df['unique_id'] = df['unique_id'].astype('string')\n",
    "        # Store the original dataframe\n",
    "        metrics[period_name][horizon] = df\n",
    "        \n",
    "        # Apply the filter_high_mape function\n",
    "        filtered_df = filter_high_mape(df, Y_test_df)\n",
    "        \n",
    "        # Store the filtered dataframe\n",
    "        filtered_metrics[period_name][horizon] = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73737082-9c74-4905-bac9-23b30bbcdda4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.225990Z",
     "iopub.status.busy": "2024-10-30T15:21:07.225418Z",
     "iopub.status.idle": "2024-10-30T15:21:07.299591Z",
     "shell.execute_reply": "2024-10-30T15:21:07.298899Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.225946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the summarized filtered metrics for each period\n",
    "metric_results_filtered = {}\n",
    "\n",
    "# Loop over each period to summarize the filtered metrics\n",
    "for period_name in periods:\n",
    "    # Retrieve the four filtered dataframes for the current period\n",
    "    df1 = filtered_metrics[period_name]['1_day']\n",
    "    df2 = filtered_metrics[period_name]['7_day']\n",
    "    df3 = filtered_metrics[period_name]['14_day']\n",
    "    df4 = filtered_metrics[period_name]['30_day']\n",
    "    \n",
    "    # Summarize the metrics using ts_utils.summarize_metrics()\n",
    "    result = ts_utils.summarize_metrics(df1, df2, df3, df4)\n",
    "    \n",
    "    # Store the result\n",
    "    metric_results_filtered[period_name] = result\n",
    "    print(f\"Finished summarizing metrics for {period_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81056e-f812-414d-acd7-3b78184f9c61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.300679Z",
     "iopub.status.busy": "2024-10-30T15:21:07.300282Z",
     "iopub.status.idle": "2024-10-30T15:21:07.323819Z",
     "shell.execute_reply": "2024-10-30T15:21:07.323054Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.300657Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_results_filtered['period01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21c6e0-d81b-43af-bf78-369b513a3b98",
   "metadata": {},
   "source": [
    "### 4.1.2 Average and standard deviation over periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524bd4a-7ee7-48e4-bc07-0d934b18ea55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.324779Z",
     "iopub.status.busy": "2024-10-30T15:21:07.324560Z",
     "iopub.status.idle": "2024-10-30T15:21:07.336287Z",
     "shell.execute_reply": "2024-10-30T15:21:07.335518Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.324757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Collect the dataframes into a list\n",
    "print(\"Collecting dataframes from 'metric_results_filtered'...\")\n",
    "dfs = []\n",
    "for period_name in periods:\n",
    "    df_styler = metric_results_filtered[period_name]\n",
    "    df = df_styler.data  # Extract the DataFrame from the Styler object\n",
    "    print(f\"  Collected dataframe for {period_name} with shape {df.shape}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# Step 2: Concatenate the dataframes along a new axis with keys as period names\n",
    "print(\"\\nConcatenating dataframes and adding 'period' as a new index level...\")\n",
    "combined_df = pd.concat(dfs, keys=periods, names=['period'])\n",
    "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "print(\"Combined dataframe index levels:\", combined_df.index.names)\n",
    "print(\"Combined dataframe columns:\", combined_df.columns.tolist())\n",
    "\n",
    "# Step 3: Compute the mean and standard deviation across periods for each metric and horizon\n",
    "print(\"\\nComputing mean and standard deviation across periods...\")\n",
    "mean_filtered_metrics = combined_df.groupby(['metric', 'horizon']).mean()\n",
    "stds_metrics = combined_df.groupby(['metric', 'horizon']).std()\n",
    "\n",
    "print(f\"Filtered metrics (mean) dataframe shape: {mean_filtered_metrics.shape}\")\n",
    "print(f\"Standard deviations dataframe shape: {stds_metrics.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c58e1-798b-4520-96c0-cb2b733f156d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.337219Z",
     "iopub.status.busy": "2024-10-30T15:21:07.336995Z",
     "iopub.status.idle": "2024-10-30T15:21:07.364651Z",
     "shell.execute_reply": "2024-10-30T15:21:07.363974Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.337197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply styling to the averaged metrics when displaying\n",
    "line_separator = [{'selector': 'tr', 'props': [('border-bottom', '1px solid black')]}]\n",
    "styled_filtered_metrics = mean_filtered_metrics.style.highlight_min(color='palegreen', axis=1)\n",
    "styled_filtered_metrics = styled_filtered_metrics.set_table_styles(line_separator, overwrite=False)\n",
    "display(styled_filtered_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf808c-ded6-4e37-8247-37abd40174df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.365620Z",
     "iopub.status.busy": "2024-10-30T15:21:07.365310Z",
     "iopub.status.idle": "2024-10-30T15:21:07.376348Z",
     "shell.execute_reply": "2024-10-30T15:21:07.375575Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.365597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show also the standard deviations of the metrics\n",
    "styled_stds_metrics = stds_metrics.style.set_table_styles(line_separator, overwrite=False)\n",
    "display(styled_stds_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd7edb-1efa-40c9-8f02-084b263d25e7",
   "metadata": {},
   "source": [
    "### 4.2 Plot the metric distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183e885-d891-4789-ab9d-f075d01a1adf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.377538Z",
     "iopub.status.busy": "2024-10-30T15:21:07.377201Z",
     "iopub.status.idle": "2024-10-30T15:21:07.585376Z",
     "shell.execute_reply": "2024-10-30T15:21:07.584198Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.377514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over each horizon\n",
    "for horizon in horizons:\n",
    "    print(f\"\\nProcessing horizon: {horizon}\")\n",
    "    \n",
    "    # Collect the set of unique_ids present in each period for the current horizon\n",
    "    unique_ids_per_period = []\n",
    "    for period in periods:\n",
    "        df = filtered_metrics[period][horizon]\n",
    "        unique_ids = set(df['unique_id'])\n",
    "        print(f\"  Period {period} has {len(unique_ids)} unique_ids for horizon {horizon}\")\n",
    "        unique_ids_per_period.append(unique_ids)\n",
    "    \n",
    "    # Find unique_ids present in all periods\n",
    "    common_unique_ids = set.intersection(*unique_ids_per_period)\n",
    "    print(f\"  {len(common_unique_ids)} unique_ids are present in all periods for horizon {horizon}\")\n",
    "    print(f\"  Unique IDs present in all periods: {sorted(common_unique_ids)}\")\n",
    "    \n",
    "    # Remove unique_ids not present in all periods from each period's dataframe\n",
    "    dfs = []\n",
    "    for period in periods:\n",
    "        df = filtered_metrics[period][horizon]\n",
    "        # Filter to keep only common_unique_ids\n",
    "        df_filtered = df[df['unique_id'].isin(common_unique_ids)].copy()\n",
    "        # Add period identifier\n",
    "        df_filtered['period'] = period\n",
    "        dfs.append(df_filtered)\n",
    "        print(f\"    After filtering, period {period} has {len(df_filtered)} rows for horizon {horizon}\")\n",
    "    \n",
    "    # Concatenate dataframes from all periods for the current horizon\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"  Combined dataframe for horizon {horizon} has {len(combined_df)} rows\")\n",
    "    \n",
    "    # Reshape the dataframe to compute mean and std for each unique_id, metric, and model\n",
    "    # Identify model columns (exclude 'unique_id', 'metric', and 'period')\n",
    "    id_vars = ['unique_id', 'metric', 'period']\n",
    "    model_columns = [col for col in combined_df.columns if col not in id_vars]\n",
    "    print(f\"  Models considered: {model_columns}\")\n",
    "    \n",
    "    # Melt the dataframe to have models in one column\n",
    "    melted_df = pd.melt(combined_df, id_vars=id_vars, value_vars=model_columns,\n",
    "                        var_name='model', value_name='value')\n",
    "    print(f\"  Melted dataframe has {len(melted_df)} rows\")\n",
    "    \n",
    "    # Group by 'unique_id', 'metric', 'model' and compute mean and std over periods\n",
    "    grouped = melted_df.groupby(['unique_id', 'metric', 'model'])['value'].agg(['mean', 'std']).reset_index()\n",
    "    print(f\"  Computed mean and std for {len(grouped)} groups\")\n",
    "    \n",
    "    # Pivot the grouped dataframe to get models as columns\n",
    "    # For mean values\n",
    "    mean_df = grouped.pivot_table(index=['unique_id', 'metric'], columns='model', values='mean').reset_index()\n",
    "    # For standard deviation values\n",
    "    std_df = grouped.pivot_table(index=['unique_id', 'metric'], columns='model', values='std').reset_index()\n",
    "    \n",
    "    # Store the dataframes with appropriate variable names\n",
    "    if horizon == '1_day':\n",
    "        eval_1_day_filtered = mean_df\n",
    "        eval_1_day_filtered_std = std_df\n",
    "    elif horizon == '7_day':\n",
    "        eval_7_days_filtered = mean_df\n",
    "        eval_7_days_filtered_std = std_df\n",
    "    elif horizon == '14_day':\n",
    "        eval_14_days_filtered = mean_df\n",
    "        eval_14_days_filtered_std = std_df\n",
    "    elif horizon == '30_day':\n",
    "        eval_30_days_filtered = mean_df\n",
    "        eval_30_days_filtered_std = std_df\n",
    "    \n",
    "    print(f\"  Stored mean and std dataframes for horizon {horizon}\")\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f4f9f-80c2-4484-9e2c-dd5ebab60cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:07.586839Z",
     "iopub.status.busy": "2024-10-30T15:21:07.586524Z",
     "iopub.status.idle": "2024-10-30T15:21:10.177496Z",
     "shell.execute_reply": "2024-10-30T15:21:10.176841Z",
     "shell.execute_reply.started": "2024-10-30T15:21:07.586817Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The models of which I want to see the metric distribution\n",
    "models_to_plot = ['ARIMA', 'PatchTST', 'Chronos (large)']\n",
    "\n",
    "# The metric of which I want to see the distribution of\n",
    "metric = 'mape'\n",
    "\n",
    "# Horizons to include in the plots\n",
    "horizons = ['7 days', '14 days', '30 days']\n",
    "\n",
    "# The evaluation dataframes\n",
    "eval_dfs = ts_utils.define_eval_dfs(eval_1_day_filtered, eval_7_days_filtered, \n",
    "                                    eval_14_days_filtered, eval_30_days_filtered, horizons)\n",
    "\n",
    "# Plot the distributions\n",
    "ts_utils.plot_metric_distribution(eval_dfs, models_to_plot, metric, Y_test_df, horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e901c84-30e3-4a41-a3f2-b170603699a9",
   "metadata": {},
   "source": [
    "### 4.3 Plotting metrics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e9b3b-083e-416b-9f28-cf2b8a1f032a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:10.178617Z",
     "iopub.status.busy": "2024-10-30T15:21:10.178298Z",
     "iopub.status.idle": "2024-10-30T15:21:10.190969Z",
     "shell.execute_reply": "2024-10-30T15:21:10.189704Z",
     "shell.execute_reply.started": "2024-10-30T15:21:10.178593Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics_academic_style(mean_df, std_df):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Ensure dataframes are correctly formatted\n",
    "    if isinstance(mean_df, pd.io.formats.style.Styler):\n",
    "        mean_df = mean_df.data\n",
    "    if isinstance(std_df, pd.io.formats.style.Styler):\n",
    "        std_df = std_df.data\n",
    "    \n",
    "    # Get list of metrics\n",
    "    metrics = mean_df.index.get_level_values('metric').unique()\n",
    "    \n",
    "    # Get list of horizons and map them to numerical values\n",
    "    horizon_labels = mean_df.index.get_level_values('horizon').unique()\n",
    "    # Convert horizons to numerical days\n",
    "    horizon_mapping = {'01 day':1, '07 days':7, '14 days':14, '30 days':30}\n",
    "    horizons = [horizon_mapping[h] for h in horizon_labels]\n",
    "    \n",
    "    # Get list of models\n",
    "    models = mean_df.columns.tolist()\n",
    "    models = ['Naive', 'Chronos (large)']\n",
    "    \n",
    "    # Set up plot styles\n",
    "    plt.rcParams.update({'font.size': 12})  # Increase font size for readability\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'P', '*', 'X']  # Unique markers for up to 8 models\n",
    "    linestyles = ['-', '--', '-.', ':']  # Different line styles\n",
    "    colors = ['black', 'blue', 'green', 'red', 'purple', 'brown', 'orange', 'gray']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        # Extract data for the current metric\n",
    "        metric_mean = mean_df.loc[metric]\n",
    "        metric_std = std_df.loc[metric]\n",
    "        \n",
    "        # For each model, plot the mean and standard deviation over horizons\n",
    "        for j, model in enumerate(models):\n",
    "            # Get mean and std values for this model\n",
    "            mean_values = metric_mean[model].values\n",
    "            std_values = metric_std[model].values\n",
    "            \n",
    "            # Plot the mean values with error bars\n",
    "            ax.errorbar(horizons, mean_values, yerr=std_values, label=model,\n",
    "                        color=colors[j % len(colors)],\n",
    "                        marker=markers[j % len(markers)],\n",
    "                        linestyle=linestyles[j % len(linestyles)],\n",
    "                        linewidth=1.5, markersize=6, capsize=3, elinewidth=1)\n",
    "            \n",
    "        # Set labels and title\n",
    "        ax.set_title(f'{metric.upper()} over Forecasting Horizons', fontsize=14)\n",
    "        ax.set_xlabel('Forecasting Horizon (days)', fontsize=12)\n",
    "        ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "        ax.set_xticks(horizons)\n",
    "        ax.set_xticklabels([str(h) for h in horizons])\n",
    "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "    \n",
    "    # Create a single legend for all subplots\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(models), bbox_to_anchor=(0.5, 1.02), fontsize=12)\n",
    "    \n",
    "    # Save the figure with high resolution\n",
    "    plt.savefig('metrics_over_time.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb845d9-d697-45ac-9225-39df19f746c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:10.192028Z",
     "iopub.status.busy": "2024-10-30T15:21:10.191807Z",
     "iopub.status.idle": "2024-10-30T15:21:12.007594Z",
     "shell.execute_reply": "2024-10-30T15:21:12.006540Z",
     "shell.execute_reply.started": "2024-10-30T15:21:10.192006Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics_academic_style(mean_filtered_metrics, stds_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dfb3b-abbe-4a51-ab42-ba7c37d759c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:12.008741Z",
     "iopub.status.busy": "2024-10-30T15:21:12.008502Z",
     "iopub.status.idle": "2024-10-30T15:21:12.889775Z",
     "shell.execute_reply": "2024-10-30T15:21:12.889018Z",
     "shell.execute_reply.started": "2024-10-30T15:21:12.008719Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_utils.plot_metrics_over_time(mean_filtered_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901000c-ea8e-4812-8116-59abe4c24ee1",
   "metadata": {},
   "source": [
    "### 4.4 Plotting model metrics against baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa775231-119c-4422-8575-68accb6a9144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:12.890887Z",
     "iopub.status.busy": "2024-10-30T15:21:12.890553Z",
     "iopub.status.idle": "2024-10-30T15:21:14.256015Z",
     "shell.execute_reply": "2024-10-30T15:21:14.254979Z",
     "shell.execute_reply.started": "2024-10-30T15:21:12.890862Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model to compare against baselines\n",
    "model = 'Chronos (large)'\n",
    "\n",
    "# Metric to plot (can be mae, mape, rmse or rmsse)\n",
    "metric = 'mape'\n",
    "\n",
    "# Baselines to use (has to be 3)\n",
    "baselines = ['PatchTST', 'NHITS', 'TimesNet']\n",
    "\n",
    "# Function for plotting\n",
    "ts_utils.compare_single_model_rmsse(eval_30_days_filtered, model, metric, 30, baselines, outlier_percentage=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fad02-92aa-4f1a-b5e4-ddb528df871a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:14.257225Z",
     "iopub.status.busy": "2024-10-30T15:21:14.256912Z",
     "iopub.status.idle": "2024-10-30T15:21:15.386873Z",
     "shell.execute_reply": "2024-10-30T15:21:15.385872Z",
     "shell.execute_reply.started": "2024-10-30T15:21:14.257201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model to compare against baselines\n",
    "model = 'Chronos (large)'\n",
    "\n",
    "# Metric to plot (can be mae, mape, rmse or rmsse)\n",
    "metric = 'mape'\n",
    "\n",
    "# Baselines to use (has to be 3)\n",
    "baselines = ['Naive', 'ARIMA', 'ETS']\n",
    "\n",
    "# Function for plotting\n",
    "ts_utils.compare_single_model_rmsse(eval_30_days_filtered, model, metric, 30, baselines, outlier_percentage=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f625d97-2ff1-4bcc-8c68-6a85d0440d45",
   "metadata": {},
   "source": [
    "## 4.5 Plotting clustered performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee9a5f-b519-47e7-a55d-7089662072eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:28.780198Z",
     "iopub.status.busy": "2024-10-30T15:21:28.779772Z",
     "iopub.status.idle": "2024-10-30T15:21:30.735755Z",
     "shell.execute_reply": "2024-10-30T15:21:30.734684Z",
     "shell.execute_reply.started": "2024-10-30T15:21:28.780173Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the groups\n",
    "group_df = ts_utils.GroupSeries(in_sample_data, num_groups=8, group_method='ApproxEntropy')\n",
    "\n",
    "# Merge the groups with the 4 dataframes\n",
    "eval_1_day_grouped = eval_1_day_filtered.merge(group_df, on='unique_id', how='left')\n",
    "eval_7_days_grouped = eval_7_days_filtered.merge(group_df, on='unique_id', how='left')\n",
    "eval_14_days_grouped = eval_14_days_filtered.merge(group_df, on='unique_id', how='left')\n",
    "eval_30_days_grouped = eval_30_days_filtered.merge(group_df, on='unique_id', how='left')\n",
    "\n",
    "eval_1_day_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54049be-6dbe-4905-a739-eb922c6aa9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:30.783445Z",
     "iopub.status.busy": "2024-10-30T15:21:30.782274Z",
     "iopub.status.idle": "2024-10-30T15:21:31.689132Z",
     "shell.execute_reply": "2024-10-30T15:21:31.688221Z",
     "shell.execute_reply.started": "2024-10-30T15:21:30.783363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the groups\n",
    "ts_utils.PlotGroups(in_sample_data, eval_30_days_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d31307-5b11-4552-bf4f-77dc51f0e8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:31.691502Z",
     "iopub.status.busy": "2024-10-30T15:21:31.691183Z",
     "iopub.status.idle": "2024-10-30T15:21:32.778733Z",
     "shell.execute_reply": "2024-10-30T15:21:32.776192Z",
     "shell.execute_reply.started": "2024-10-30T15:21:31.691479Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of dataframes and corresponding forecasting horizons\n",
    "dataframes = [eval_1_day_grouped, eval_7_days_grouped, eval_14_days_grouped, eval_30_days_grouped]\n",
    "horizons = [1, 7, 14, 30]\n",
    "\n",
    "# Models we want to plot\n",
    "models = ['Naive', 'TimesNet', 'PatchTST', 'NHITS', 'Chronos (small)', 'Chronos (large)']\n",
    "\n",
    "ts_utils.PlotGroupPerformance(dataframes, models, horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdbe59-78f0-40e5-8436-b5bf1468516a",
   "metadata": {},
   "source": [
    "## 4.6 In vs Out-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea689156-df4a-4252-866f-7f5c8d5079c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:32.782237Z",
     "iopub.status.busy": "2024-10-30T15:21:32.781113Z",
     "iopub.status.idle": "2024-10-30T15:21:32.812512Z",
     "shell.execute_reply": "2024-10-30T15:21:32.811430Z",
     "shell.execute_reply.started": "2024-10-30T15:21:32.782164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the data to the long format\n",
    "Y_df_out = out_sample_data.melt(id_vars=['date'], var_name='unique_id', value_name='y')\n",
    "Y_df_out = Y_df_out.rename(columns={'date':'ds'})\n",
    "\n",
    "# Convert date column to datetime type\n",
    "Y_df_out['ds'] = pd.to_datetime(Y_df_out['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdd4e4-3e52-401c-b082-1cf27ec4cca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:32.813840Z",
     "iopub.status.busy": "2024-10-30T15:21:32.813583Z",
     "iopub.status.idle": "2024-10-30T15:21:32.824281Z",
     "shell.execute_reply": "2024-10-30T15:21:32.823147Z",
     "shell.execute_reply.started": "2024-10-30T15:21:32.813816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the horizon (12 months of 30 days each)\n",
    "fh = 30\n",
    "horizon = 12 * fh\n",
    "\n",
    "# Identify the unique dates in the dataset\n",
    "unique_dates = Y_df_out['ds'].unique()\n",
    "\n",
    "# Convert to a list and then sort the dates\n",
    "unique_dates = sorted(list(unique_dates))\n",
    "\n",
    "# Determine the cutoff date (cutoff at 12 months before the last date in the dataset)\n",
    "cutoff_date = unique_dates[-(horizon + 1)]\n",
    "\n",
    "# Training data: all data up to the cutoff date\n",
    "Y_train_df_out = Y_df_out[Y_df_out['ds'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a1459-b777-449c-90fb-e2347013522e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:32.825647Z",
     "iopub.status.busy": "2024-10-30T15:21:32.825362Z",
     "iopub.status.idle": "2024-10-30T15:21:32.861663Z",
     "shell.execute_reply": "2024-10-30T15:21:32.859873Z",
     "shell.execute_reply.started": "2024-10-30T15:21:32.825616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store the input and test sets\n",
    "input_dfs_out = []\n",
    "test_dfs_out = []\n",
    "\n",
    "# Loop to create the 6 input and test sets\n",
    "for i in range(6):\n",
    "    # Determine the start date of the test period\n",
    "    test_start_date = unique_dates[-(horizon - i * 2 * fh)]\n",
    "    test_end_date = unique_dates[-(horizon - (i * 2 * fh) - fh)]\n",
    "    \n",
    "    # Input data: all data up to the start of the current test period\n",
    "    input_df = Y_df_out[Y_df_out['ds'] <= test_start_date]\n",
    "    input_dfs_out.append(input_df)\n",
    "    \n",
    "    # Test data: the 30-day period following the start of the test period\n",
    "    test_df = Y_df_out[(Y_df_out['ds'] > test_start_date) & (Y_df_out['ds'] <= test_end_date)]\n",
    "    test_dfs_out.append(test_df)\n",
    "\n",
    "# Define the 6 input periods\n",
    "Y_input_df_0_out = input_dfs_out[0]\n",
    "Y_input_df_1_out = input_dfs_out[1]\n",
    "Y_input_df_2_out = input_dfs_out[2]\n",
    "Y_input_df_3_out = input_dfs_out[3]\n",
    "Y_input_df_4_out = input_dfs_out[4]\n",
    "Y_input_df_5_out = input_dfs_out[5]\n",
    "\n",
    "# Define the 6 test periods\n",
    "Y_test_df_0_out = test_dfs_out[0]\n",
    "Y_test_df_1_out = test_dfs_out[1]\n",
    "Y_test_df_2_out = test_dfs_out[2]\n",
    "Y_test_df_3_out = test_dfs_out[3]\n",
    "Y_test_df_4_out = test_dfs_out[4]\n",
    "Y_test_df_5_out = test_dfs_out[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b6c67-5169-4b1f-8d58-8403ec2643a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:32.863256Z",
     "iopub.status.busy": "2024-10-30T15:21:32.862888Z",
     "iopub.status.idle": "2024-10-30T15:21:33.026172Z",
     "shell.execute_reply": "2024-10-30T15:21:33.025049Z",
     "shell.execute_reply.started": "2024-10-30T15:21:32.863231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define periods and horizons\n",
    "periods = [f'period{i+1:02d}' for i in range(6)]  # ['period01', 'period02', ..., 'period06']\n",
    "horizons = ['1_day', '7_day', '14_day', '30_day']\n",
    "\n",
    "# Initialize a nested dictionary to store dataframes\n",
    "metrics_out = {}\n",
    "\n",
    "# Loop over periods and horizons to read the dataframes\n",
    "for period_name in periods:\n",
    "    metrics_out[period_name] = {}\n",
    "    for horizon in horizons:\n",
    "        # Construct the filename\n",
    "        filename = f\"metrics/outsample/{period_name}/metrics_{horizon}_{data_date}_{data_size}.csv\"\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        # Ensure 'unique_id' is of type string\n",
    "        df['unique_id'] = df['unique_id'].astype('string')\n",
    "        # Store the dataframe\n",
    "        metrics_out[period_name][horizon] = df\n",
    "\n",
    "# Initialize a dictionary to store the summarized metrics for each period\n",
    "metric_results_out = {}\n",
    "\n",
    "# Loop over each period to summarize the metrics\n",
    "for period_name in periods:\n",
    "    # Retrieve the four dataframes for the current period\n",
    "    df1 = metrics_out[period_name]['1_day']\n",
    "    df2 = metrics_out[period_name]['7_day']\n",
    "    df3 = metrics_out[period_name]['14_day']\n",
    "    df4 = metrics_out[period_name]['30_day']\n",
    "    \n",
    "    # Summarize the metrics using ts_utils.summarize_metrics()\n",
    "    result = ts_utils.summarize_metrics(df1, df2, df3, df4)\n",
    "    \n",
    "    # Store the result\n",
    "    metric_results_out[period_name] = result\n",
    "\n",
    "metric_results_out['period01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf663f-3be9-49e1-8a13-844863e26155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.058396Z",
     "iopub.status.busy": "2024-10-30T15:21:33.057985Z",
     "iopub.status.idle": "2024-10-30T15:21:33.062580Z",
     "shell.execute_reply": "2024-10-30T15:21:33.061908Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.058370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize nested dictionaries to store original and filtered dataframes\n",
    "metrics_out = {}\n",
    "filtered_metrics_out = {}\n",
    "\n",
    "# Create a mapping of period names to their corresponding Y_test_df\n",
    "Y_test_dfs_out = {f'period{i+1:02d}': test_dfs_out[i] for i in range(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf85275-a35d-4e84-bda2-c3434efb7212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.063497Z",
     "iopub.status.busy": "2024-10-30T15:21:33.063272Z",
     "iopub.status.idle": "2024-10-30T15:21:33.157986Z",
     "shell.execute_reply": "2024-10-30T15:21:33.156913Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.063476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over periods and horizons to read the dataframes and apply filtering\n",
    "for period_name in periods:\n",
    "    print(f\"Processing {period_name}...\")\n",
    "    metrics_out[period_name] = {}\n",
    "    filtered_metrics_out[period_name] = {}\n",
    "    \n",
    "    # Get the corresponding Y_test_df for the current period\n",
    "    Y_test_df = Y_test_dfs_out[period_name]\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        # Construct the filename\n",
    "        filename = f\"metrics/outsample/{period_name}/metrics_{horizon}_{data_date}_{data_size}.csv\"\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Ensure 'unique_id' is of type string\n",
    "        df['unique_id'] = df['unique_id'].astype('string')\n",
    "\n",
    "        # Store the original dataframe\n",
    "        metrics_out[period_name][horizon] = df\n",
    "        \n",
    "        # Apply the filter_high_mape function\n",
    "        filtered_df = filter_high_mape(df, Y_test_df)\n",
    "        \n",
    "        # Store the filtered dataframe\n",
    "        filtered_metrics_out[period_name][horizon] = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66084281-46e5-4163-9ec0-d0fd92ace51f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.159145Z",
     "iopub.status.busy": "2024-10-30T15:21:33.158919Z",
     "iopub.status.idle": "2024-10-30T15:21:33.227860Z",
     "shell.execute_reply": "2024-10-30T15:21:33.226908Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.159124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the summarized filtered metrics for each period\n",
    "metric_results_filtered_out = {}\n",
    "\n",
    "# Loop over each period to summarize the filtered metrics\n",
    "for period_name in periods:\n",
    "    # Retrieve the four filtered dataframes for the current period\n",
    "    df1 = filtered_metrics_out[period_name]['1_day']\n",
    "    df2 = filtered_metrics_out[period_name]['7_day']\n",
    "    df3 = filtered_metrics_out[period_name]['14_day']\n",
    "    df4 = filtered_metrics_out[period_name]['30_day']\n",
    "    \n",
    "    # Summarize the metrics using ts_utils.summarize_metrics()\n",
    "    result = ts_utils.summarize_metrics(df1, df2, df3, df4)\n",
    "    \n",
    "    # Store the result\n",
    "    metric_results_filtered_out[period_name] = result\n",
    "    print(f\"Finished summarizing metrics for {period_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00da218-2215-4c3a-951e-e7dfb07a872a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.230728Z",
     "iopub.status.busy": "2024-10-30T15:21:33.230247Z",
     "iopub.status.idle": "2024-10-30T15:21:33.241798Z",
     "shell.execute_reply": "2024-10-30T15:21:33.241141Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.230701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Collect the dataframes into a list\n",
    "print(\"Collecting dataframes from 'metric_results_filtered'...\")\n",
    "dfs = []\n",
    "for period_name in periods:\n",
    "    df_styler = metric_results_filtered_out[period_name]\n",
    "    df = df_styler.data  # Extract the DataFrame from the Styler object\n",
    "    print(f\"  Collected dataframe for {period_name} with shape {df.shape}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# Step 2: Concatenate the dataframes along a new axis with keys as period names\n",
    "print(\"\\nConcatenating dataframes and adding 'period' as a new index level...\")\n",
    "combined_df = pd.concat(dfs, keys=periods, names=['period'])\n",
    "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "print(\"Combined dataframe index levels:\", combined_df.index.names)\n",
    "print(\"Combined dataframe columns:\", combined_df.columns.tolist())\n",
    "\n",
    "# Step 3: Compute the mean and standard deviation across periods for each metric and horizon\n",
    "print(\"\\nComputing mean and standard deviation across periods...\")\n",
    "mean_filtered_metrics_out = combined_df.groupby(['metric', 'horizon']).mean()\n",
    "stds_metrics_out = combined_df.groupby(['metric', 'horizon']).std()\n",
    "\n",
    "print(f\"Filtered metrics (mean) dataframe shape: {mean_filtered_metrics_out.shape}\")\n",
    "print(f\"Standard deviations dataframe shape: {stds_metrics_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330f3a9-8655-46fc-b8cf-45718fb953e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.242680Z",
     "iopub.status.busy": "2024-10-30T15:21:33.242461Z",
     "iopub.status.idle": "2024-10-30T15:21:33.273876Z",
     "shell.execute_reply": "2024-10-30T15:21:33.271516Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.242659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply styling to the averaged metrics when displaying\n",
    "line_separator = [{'selector': 'tr', 'props': [('border-bottom', '1px solid black')]}]\n",
    "styled_filtered_metrics_out = mean_filtered_metrics_out.style.highlight_min(color='palegreen', axis=1)\n",
    "styled_filtered_metrics_out = styled_filtered_metrics_out.set_table_styles(line_separator, overwrite=False)\n",
    "display(styled_filtered_metrics_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38faab-abd7-495d-9ade-f7dfaac448df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.277046Z",
     "iopub.status.busy": "2024-10-30T15:21:33.276750Z",
     "iopub.status.idle": "2024-10-30T15:21:33.288480Z",
     "shell.execute_reply": "2024-10-30T15:21:33.287696Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.277019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show also the standard deviations of the metrics\n",
    "styled_stds_metrics_out = stds_metrics_out.style.set_table_styles(line_separator, overwrite=False)\n",
    "display(styled_stds_metrics_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd15afa-df5c-4185-bcd0-f6099a453549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.289531Z",
     "iopub.status.busy": "2024-10-30T15:21:33.289197Z",
     "iopub.status.idle": "2024-10-30T15:21:33.294236Z",
     "shell.execute_reply": "2024-10-30T15:21:33.293327Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.289509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model categories\n",
    "model_categories = {\n",
    "    'Naive': 'Baseline Model',\n",
    "    'ARIMA': 'Baseline Model',\n",
    "    'ETS': 'Baseline Model',\n",
    "    'ES_bu': 'Baseline Model',\n",
    "    'PatchTST': 'Deep Model',\n",
    "    'NHITS': 'Deep Model',\n",
    "    'DeepAR': 'Deep Model',\n",
    "    'TimesNet': 'Deep Model',\n",
    "    'ESRNN': 'Deep Model',\n",
    "    'Chronos (small)': 'Pre-trained Model',\n",
    "    'Chronos (large)': 'Pre-trained Model',\n",
    "    'Chronos (FT)': 'Pre-trained Model',\n",
    "    'Chronos': 'Pre-trained Model',\n",
    "    'TimesFM': 'Pre-trained Model',\n",
    "    'TimesFM (FT)': 'Pre-trained Model'\n",
    "}\n",
    "\n",
    "# Define lighter colors for each category\n",
    "colors = {\n",
    "    'Deep Model': '#FFA07A',  # light salmon\n",
    "    'Pre-trained Model': '#9370DB',  # medium purple\n",
    "    'Baseline Model': '#ADD8E6'  # light blue\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df149ca3-f941-45e2-88b1-c4beda7ca2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.295237Z",
     "iopub.status.busy": "2024-10-30T15:21:33.294968Z",
     "iopub.status.idle": "2024-10-30T15:21:33.663247Z",
     "shell.execute_reply": "2024-10-30T15:21:33.662281Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.295215Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_utils.Plotting_MAPE(styled_filtered_metrics, styled_filtered_metrics_out, model_categories, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a4340-7445-411d-bc02-2b0b9544052b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.664430Z",
     "iopub.status.busy": "2024-10-30T15:21:33.664128Z",
     "iopub.status.idle": "2024-10-30T15:21:33.679814Z",
     "shell.execute_reply": "2024-10-30T15:21:33.678802Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.664399Z"
    }
   },
   "outputs": [],
   "source": [
    "def Plotting_MAPE(df1, df2, std_df1, std_df2, model_categories, colors):\n",
    "    # Convert data to DataFrame if it's a Styler object\n",
    "    if isinstance(df1, pd.io.formats.style.Styler):\n",
    "        df1 = df1.data\n",
    "    if isinstance(df2, pd.io.formats.style.Styler):\n",
    "        df2 = df2.data\n",
    "    if isinstance(std_df1, pd.io.formats.style.Styler):\n",
    "        std_df1 = std_df1.data\n",
    "    if isinstance(std_df2, pd.io.formats.style.Styler):\n",
    "        std_df2 = std_df2.data\n",
    "    \n",
    "    # Extract MAPE data for '30 days' horizon and sort\n",
    "    mape_30d_df1 = df1.loc[('mape', '30 days')].sort_values(ascending=True)\n",
    "    mape_std_30d_df1 = std_df1.loc[('mape', '30 days')][mape_30d_df1.index]\n",
    "    \n",
    "    mape_30d_df2 = df2.loc[('mape', '30 days')].sort_values(ascending=True)\n",
    "    mape_std_30d_df2 = std_df2.loc[('mape', '30 days')][mape_30d_df2.index]\n",
    "    \n",
    "    models_df1 = mape_30d_df1.index\n",
    "    models_df2 = mape_30d_df2.index\n",
    "    \n",
    "    num_models = len(df1.columns)\n",
    "\n",
    "    # Plotting setup\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(num_models * 14/8, num_models * 5/8))\n",
    "    \n",
    "    # Error bar config\n",
    "    error_config = {'ecolor': 'black', 'elinewidth': 1, 'capsize': 3}\n",
    "    \n",
    "    # Plotting loop for in-sample and out-sample MAPE\n",
    "    for ax, models, mape_data, mape_std_data, xlabel in zip(\n",
    "            axes, \n",
    "            [models_df1, models_df2], \n",
    "            [mape_30d_df1, mape_30d_df2], \n",
    "            [mape_std_30d_df1, mape_std_30d_df2],\n",
    "            ['In-sample MAPE', 'Out-sample MAPE']):\n",
    "        \n",
    "        colors_list = [colors[model_categories[model]] for model in models]\n",
    "        \n",
    "        # Plot horizontal bars with error bars\n",
    "        bars = ax.barh(models, mape_data.values, xerr=mape_std_data.values, color=colors_list,\n",
    "                       error_kw=error_config)\n",
    "        \n",
    "        # Set labels and invert y-axis\n",
    "        ax.set_xlabel(xlabel, fontsize=14, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Annotate bars with MAPE values and standard deviations\n",
    "        for i, (v, std) in enumerate(zip(mape_data.values, mape_std_data.values)):\n",
    "            ax.text(v + std + 0.02 * max(mape_data.values), i, f'{v:.2f}', \n",
    "                    color='black', va='center', fontsize=10)\n",
    "        \n",
    "        # Adjust x-axis limits to accommodate error bars and annotations\n",
    "        max_x = max(mape_data.values + mape_std_data.values)\n",
    "        ax.set_xlim(0, max_x + 0.2 * max_x)\n",
    "    \n",
    "    # Adjust the plot layout\n",
    "    plt.subplots_adjust(left=0.3, right=0.95, top=0.85, bottom=0.1)\n",
    "    \n",
    "    # Adding legend for model categories\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color) for color in colors.values()]\n",
    "    labels = colors.keys()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(labels))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89c2af-54e6-4949-ba7c-d70a4a61e2f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T15:21:33.681609Z",
     "iopub.status.busy": "2024-10-30T15:21:33.681237Z",
     "iopub.status.idle": "2024-10-30T15:21:34.082808Z",
     "shell.execute_reply": "2024-10-30T15:21:34.081798Z",
     "shell.execute_reply.started": "2024-10-30T15:21:33.681580Z"
    }
   },
   "outputs": [],
   "source": [
    "Plotting_MAPE(styled_filtered_metrics, styled_filtered_metrics_out, styled_stds_metrics, styled_stds_metrics_out, model_categories, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22d2ce-0687-4c97-8f2f-4e479d4d3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_entropy_vs_metric(df, model_names):\n",
    "    # Filter the DataFrame for metric == 'mape'\n",
    "    df_filtered = df[df['metric'] == 'mape']\n",
    "\n",
    "    # Select only the columns we need\n",
    "    columns_to_select = ['ApproxEntropy'] + model_names\n",
    "    df_selected = df_filtered[columns_to_select]\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = df_selected.melt(\n",
    "        id_vars=['ApproxEntropy'],\n",
    "        value_vars=model_names,\n",
    "        var_name='Model',\n",
    "        value_name='MetricValue'\n",
    "    )\n",
    "\n",
    "    # Remove NaNs\n",
    "    df_melted = df_melted.dropna(subset=['ApproxEntropy', 'MetricValue'])\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Define colors for models\n",
    "    colors = plt.cm.tab10.colors  # Use a colormap with 10 distinct colors\n",
    "    color_dict = dict(zip(model_names, colors[:len(model_names)]))\n",
    "\n",
    "    # For custom legend handles\n",
    "    legend_elements = []\n",
    "\n",
    "    # Plot scatter plots and trendlines for each model\n",
    "    for model in model_names:\n",
    "        model_data = df_melted[df_melted['Model'] == model]\n",
    "        x = model_data['ApproxEntropy']\n",
    "        y = model_data['MetricValue']\n",
    "        color = color_dict[model]\n",
    "\n",
    "        # Plot scatter points\n",
    "        plt.scatter(x, y, color=color, alpha=0.7)\n",
    "\n",
    "        # Fit regression line\n",
    "        if len(x) > 1:\n",
    "            slope, intercept = np.polyfit(x, y, 1)\n",
    "            x_vals = np.array([x.min(), x.max()])\n",
    "            y_vals = intercept + slope * x_vals\n",
    "            plt.plot(\n",
    "                x_vals,\n",
    "                y_vals,\n",
    "                color=color,\n",
    "                linestyle='--',\n",
    "                linewidth=2\n",
    "            )\n",
    "\n",
    "        # Create a custom legend handle\n",
    "        legend_element = Line2D(\n",
    "            [0], [0],\n",
    "            color=color,\n",
    "            marker='o',\n",
    "            linestyle='--',\n",
    "            markersize=6,\n",
    "            linewidth=2,\n",
    "            label=model\n",
    "        )\n",
    "        legend_elements.append(legend_element)\n",
    "\n",
    "    plt.xlabel('Approximate Entropy')\n",
    "    plt.ylabel('MAPE')\n",
    "    plt.title('Approximate Entropy vs. MAPE for Different Models')\n",
    "    plt.grid(True)\n",
    "    # plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add the legend\n",
    "    plt.legend(handles=legend_elements, loc='best', title='Models')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bab19-bda7-4188-b666-3497a8345d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['Naive', 'Chronos (large)']\n",
    "\n",
    "plot_entropy_vs_metric(eval_30_days_grouped, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2fa51-5ec1-4662-9761-6119a8c3b835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17548bc-896c-4778-b9ea-55285c092680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAP Environment",
   "language": "python",
   "name": "dap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
