{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae0b593-0b44-442d-bb29-b35b5283b0e3",
   "metadata": {},
   "source": [
    "# Timeseries Forecasting on Transaction Data\n",
    "\n",
    "## 1. Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7893b-71a9-443c-ae5b-8606851c5849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Pip for evaluation metrics\n",
    "# !pip install datasetsforecast\n",
    "# !pip install sktime\n",
    "# !pip install entropyhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12442afe-f1e1-494a-b261-828b837d04e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Some functions for plotting and stuff\n",
    "import ts_utils as ts_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ff974-b634-413d-813b-447c8fec8dce",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e7320-d2ec-45ed-8091-79ca332a368d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Size of the data to read\n",
    "data_size = 'norm'\n",
    "\n",
    "# Date of the data to read\n",
    "data_date = '2110' # '2110' = 21st of October\n",
    "\n",
    "# Read the data (takes around 2 minutes)\n",
    "dataset = pd.read_csv(f\"~/Thesis/data/eod_balances_{data_date}_{data_size}.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34465b37-99fa-453d-9ce3-5814e0e24caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:56:49.027930Z",
     "iopub.status.busy": "2024-07-02T11:56:49.026975Z",
     "iopub.status.idle": "2024-07-02T11:56:49.032924Z",
     "shell.execute_reply": "2024-07-02T11:56:49.031885Z",
     "shell.execute_reply.started": "2024-07-02T11:56:49.027866Z"
    },
    "tags": []
   },
   "source": [
    "### 2.1 In-sample and Out-sample split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370ae60-b7e3-4518-9adf-21c367743104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate total amount of timeseries\n",
    "num_timeseries = len(dataset.columns) - 1\n",
    "\n",
    "# Specify train test split percentage\n",
    "train_test_split = 0.8\n",
    "\n",
    "# Split into train and out of sample test data\n",
    "num_out_of_sample = int(train_test_split * num_timeseries)\n",
    "\n",
    "# Create in-sample dataframe\n",
    "in_sample_data = dataset.iloc[:, : num_out_of_sample + 1] # Training and testing\n",
    "\n",
    "# Create out-sample dataframe\n",
    "n = num_timeseries-num_out_of_sample\n",
    "columns_to_keep = dataset.columns[[0]].tolist() + dataset.columns[-n:].tolist()\n",
    "out_sample_data = dataset[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1d7dc-20f8-44dc-9c2b-333f2043da10",
   "metadata": {},
   "source": [
    "## 3. In-sample analysis\n",
    "\n",
    "### 3.1 Train/Test splitting and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7b589-ce7a-4751-9c8d-22bff4a941e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change the data to the long format\n",
    "Y_df = in_sample_data.melt(id_vars=['date'], var_name='unique_id', value_name='y')\n",
    "Y_df = Y_df.rename(columns={'date':'ds'})\n",
    "\n",
    "# Convert date column to datetime type\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05349a0a-acfb-4238-bddc-ec2f90ac9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the horizon (12 months of 30 days each)\n",
    "fh = 30\n",
    "horizon = 12 * fh\n",
    "\n",
    "# Identify the unique dates in the dataset\n",
    "unique_dates = Y_df['ds'].unique()\n",
    "\n",
    "# Convert to a list and then sort the dates\n",
    "unique_dates = sorted(list(unique_dates))\n",
    "\n",
    "# Determine the cutoff date (cutoff at 12 months before the last date in the dataset)\n",
    "cutoff_date = unique_dates[-(horizon + 1)]\n",
    "\n",
    "# Training data: all data up to the cutoff date\n",
    "Y_train_df = Y_df[Y_df['ds'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63408b90-0dda-42ac-afc3-aa3a6dde1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the input and test sets\n",
    "input_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Loop to create the 6 input and test sets\n",
    "for i in range(6):\n",
    "    # Determine the start date of the test period\n",
    "    test_start_date = unique_dates[-(horizon - i * 2 * fh)]\n",
    "    test_end_date = unique_dates[-(horizon - (i * 2 * fh) - fh)]\n",
    "    \n",
    "    # Input data: all data up to the start of the current test period\n",
    "    input_df = Y_df[Y_df['ds'] <= test_start_date]\n",
    "    input_dfs.append(input_df)\n",
    "    \n",
    "    # Test data: the 30-day period following the start of the test period\n",
    "    test_df = Y_df[(Y_df['ds'] > test_start_date) & (Y_df['ds'] <= test_end_date)]\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "# Define the 6 input periods\n",
    "Y_input_df_0 = input_dfs[0]\n",
    "Y_input_df_1 = input_dfs[1]\n",
    "Y_input_df_2 = input_dfs[2]\n",
    "Y_input_df_3 = input_dfs[3]\n",
    "Y_input_df_4 = input_dfs[4]\n",
    "Y_input_df_5 = input_dfs[5]\n",
    "\n",
    "# Define the 6 test periods\n",
    "Y_test_df_0 = test_dfs[0]\n",
    "Y_test_df_1 = test_dfs[1]\n",
    "Y_test_df_2 = test_dfs[2]\n",
    "Y_test_df_3 = test_dfs[3]\n",
    "Y_test_df_4 = test_dfs[4]\n",
    "Y_test_df_5 = test_dfs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96c79c-6c2d-4559-a765-3e935f13987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeserie to plot\n",
    "unique_id = '6'\n",
    "\n",
    "# Plot the train and test dataframes\n",
    "ts_utils.plot_train_test_split(Y_input_df_0, Y_test_df_0, unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dad8a6-5d1f-4d54-8052-2c9378847885",
   "metadata": {},
   "source": [
    "### 3.2 Retrieve Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdef4e-ada6-4c3e-8165-7c38fa371d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and periods\n",
    "models = ['Naive', 'ARIMA', 'ETS', 'NHITS', 'PatchTST', 'TimesNet', 'DeepAR', 'Chronos-small', 'Chronos-large', 'Chronos-FT']\n",
    "periods = ['period01', 'period02', 'period03', 'period04', 'period05', 'period06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff1d58-1708-4163-b282-f14645f9d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Y_test_dfs as a list of test dataframes for each period\n",
    "Y_test_dfs = [Y_test_df_0, Y_test_df_1, Y_test_df_2, Y_test_df_3, Y_test_df_4, Y_test_df_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336131e-6e19-4e4b-8c54-6b8764439643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the prediction dataframes for each period\n",
    "Y_pred_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d6ca4-24c5-4f27-b972-4378a3dceb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the predictions\n",
    "def merging_preds(Y_pred_df, model_preds, model_name):\n",
    "    # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "    model_preds['unique_id'] = model_preds['unique_id'].astype('string')\n",
    "    model_preds['ds'] = pd.to_datetime(model_preds['ds'])\n",
    "    \n",
    "    # Merge predictions on 'unique_id' and 'ds'\n",
    "    Y_pred_df = Y_pred_df.merge(model_preds[['unique_id', 'ds', f'{model_name}']], on=['unique_id', 'ds'], how='left')\n",
    "    \n",
    "    return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ec269-c8a6-4b86-981f-8526030a06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over periods to get predictions\n",
    "for i, period in enumerate(periods):\n",
    "    print(f\"Processing {period}...\")\n",
    "    \n",
    "    # Get the test dataframe for this period\n",
    "    Y_pred_df = Y_test_dfs[i].copy()\n",
    "    \n",
    "    # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "    Y_pred_df['unique_id'] = Y_pred_df['unique_id'].astype('string')\n",
    "    Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "    \n",
    "    # Loop over models to merge predictions\n",
    "    for model in models:\n",
    "        # Read the prediction csv\n",
    "        model_preds = pd.read_csv(f\"predictions/{model}/insample/{period}/model_preds_{data_date}_{data_size}.csv\")\n",
    "        \n",
    "        # Merge the predictions into Y_pred_df\n",
    "        Y_pred_df = merging_preds(Y_pred_df, model_preds, model)\n",
    "\n",
    "    # Set 'unique_id' as index if needed\n",
    "    Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "    \n",
    "    # Rename columns if necessary\n",
    "    Y_pred_df = Y_pred_df.rename(columns={\"Chronos-small\": \"Chronos (small)\", \"Chronos-large\": \"Chronos (large)\", \"Chronos-FT\": \"Chronos (FT)\"})\n",
    "    \n",
    "    # Store the dataframe in the dictionary\n",
    "    Y_pred_dfs[period] = Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81a087-9ad5-479c-85af-77cb5947bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_input_dfs as a list of input dataframes for each period\n",
    "Y_input_dfs = [Y_input_df_0, Y_input_df_1, Y_input_df_2, Y_input_df_3, Y_input_df_4, Y_input_df_5]\n",
    "\n",
    "# Function to plot predictions for a given unique_id\n",
    "def plot_model_predictions(unique_id, Y_input_dfs, Y_pred_dfs, model_list, history_days=100, grid_shape='3x2'):\n",
    "    # Parse grid_shape into nrows and ncols\n",
    "    if grid_shape == '3x2':\n",
    "        nrows, ncols = 3, 2\n",
    "    elif grid_shape == '6x1':\n",
    "        nrows, ncols = 6, 1\n",
    "    else:\n",
    "        # Default to 6x1 if invalid input\n",
    "        nrows, ncols = 6, 1\n",
    "\n",
    "    # Prepare to find overall x and y limits\n",
    "    all_dates = []\n",
    "    all_values = []\n",
    "\n",
    "    # Define colors for the different regions\n",
    "    colors = {\n",
    "        'train': '#a6bddb',\n",
    "        'input': '#fd8d3c',\n",
    "        'test': '#feb24c',\n",
    "    }\n",
    "\n",
    "    # Distinct model colors\n",
    "    colors_list = ['purple', 'orange', 'cyan', 'magenta', 'brown', 'red', 'green', 'olive', 'navy', 'teal']\n",
    "    \n",
    "    # Create a mapping from model names to colors\n",
    "    model_colors = dict(zip(model_list, colors_list))\n",
    "\n",
    "    # First, determine the cutoff_date (end of train data)\n",
    "    # We'll use the last date from the first Y_input_df\n",
    "    Y_input_df_first = Y_input_dfs[0].copy()\n",
    "    Y_input_df_first['ds'] = pd.to_datetime(Y_input_df_first['ds'])\n",
    "    if Y_input_df_first.index.name != 'unique_id':\n",
    "        Y_input_df_first = Y_input_df_first.set_index('unique_id')\n",
    "    try:\n",
    "        Y_input_ts_first = Y_input_df_first.loc[unique_id].copy()\n",
    "    except KeyError:\n",
    "        print(f\"unique_id '{unique_id}' not found in the first Y_input_df\")\n",
    "        return\n",
    "    if isinstance(Y_input_ts_first, pd.Series):\n",
    "        Y_input_ts_first = Y_input_ts_first.to_frame().T\n",
    "    cutoff_date = Y_input_ts_first['ds'].max()\n",
    "\n",
    "    # Calculate start date for historical data\n",
    "    history_start_date = cutoff_date - pd.Timedelta(days=history_days)\n",
    "\n",
    "    # Collect all dates and values for axis limits\n",
    "    for i, period in enumerate(periods):\n",
    "        # Get input data for the period\n",
    "        Y_input_df = Y_input_dfs[i].copy()\n",
    "        Y_input_df['ds'] = pd.to_datetime(Y_input_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_input_df.index.name != 'unique_id':\n",
    "            Y_input_df = Y_input_df.set_index('unique_id')\n",
    "\n",
    "        # Get prediction data for the period\n",
    "        Y_pred_df = Y_pred_dfs[period].copy()\n",
    "        Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_pred_df.index.name != 'unique_id':\n",
    "            Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "\n",
    "        # Filter data for the specific unique_id\n",
    "        try:\n",
    "            Y_input_ts = Y_input_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            Y_pred_ts = Y_pred_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # If the result is a Series (only one entry), convert it to DataFrame\n",
    "        if isinstance(Y_input_ts, pd.Series):\n",
    "            Y_input_ts = Y_input_ts.to_frame().T\n",
    "\n",
    "        if isinstance(Y_pred_ts, pd.Series):\n",
    "            Y_pred_ts = Y_pred_ts.to_frame().T\n",
    "\n",
    "        # Determine the start and end date of the test period for this period\n",
    "        test_start_date = Y_pred_ts['ds'].min()\n",
    "        test_end_date = Y_pred_ts['ds'].max()\n",
    "\n",
    "        # Limit historical data to the specified number of days before test_start_date\n",
    "        Y_input_ts = Y_input_ts[(Y_input_ts['ds'] >= history_start_date) & (Y_input_ts['ds'] <= test_start_date)]\n",
    "\n",
    "        # Collect dates and values\n",
    "        all_dates.extend(Y_input_ts['ds'].tolist())\n",
    "        all_values.extend(Y_input_ts['y'].tolist())\n",
    "        all_dates.extend(Y_pred_ts['ds'].tolist())\n",
    "        all_values.extend(Y_pred_ts['y'].tolist())\n",
    "\n",
    "        # Collect values from model predictions\n",
    "        for model in model_list:\n",
    "            if model in Y_pred_ts.columns:\n",
    "                all_values.extend(Y_pred_ts[model].tolist())\n",
    "\n",
    "    # Determine overall x and y limits with padding\n",
    "    x_min = min(all_dates)\n",
    "    x_max = max(all_dates)\n",
    "    y_min = min(all_values)\n",
    "    y_max = max(all_values)\n",
    "\n",
    "    # Calculate padding\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_padding = x_range * 0.05  # 5% padding on x-axis\n",
    "    y_padding = y_range * 0.05  # 5% padding on y-axis\n",
    "\n",
    "    # Adjust x_min and x_max with padding\n",
    "    x_min_padded = x_min - x_padding\n",
    "    x_max_padded = x_max + x_padding\n",
    "\n",
    "    # Adjust y_min and y_max with padding\n",
    "    y_min_padded = y_min - y_padding\n",
    "    y_max_padded = y_max + y_padding\n",
    "\n",
    "    # Create the grid of subplots based on grid_shape\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(18, 5 * nrows))\n",
    "    axs = axs.flatten()  # Flatten to easily index subplots\n",
    "\n",
    "    for i, period in enumerate(periods):\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Get input data for the period\n",
    "        Y_input_df = Y_input_dfs[i].copy()\n",
    "        Y_input_df['ds'] = pd.to_datetime(Y_input_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_input_df.index.name != 'unique_id':\n",
    "            Y_input_df = Y_input_df.set_index('unique_id')\n",
    "\n",
    "        # Get prediction data for the period\n",
    "        Y_pred_df = Y_pred_dfs[period].copy()\n",
    "        Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_pred_df.index.name != 'unique_id':\n",
    "            Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "\n",
    "        # Filter data for the specific unique_id\n",
    "        try:\n",
    "            Y_input_ts = Y_input_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            print(f\"unique_id '{unique_id}' not found in Y_input_df for period '{period}'\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            Y_pred_ts = Y_pred_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            print(f\"unique_id '{unique_id}' not found in Y_pred_df for period '{period}'\")\n",
    "            continue\n",
    "\n",
    "        # If the result is a Series (only one entry), convert it to DataFrame\n",
    "        if isinstance(Y_input_ts, pd.Series):\n",
    "            Y_input_ts = Y_input_ts.to_frame().T\n",
    "\n",
    "        if isinstance(Y_pred_ts, pd.Series):\n",
    "            Y_pred_ts = Y_pred_ts.to_frame().T\n",
    "\n",
    "        # Determine the start and end date of the test period for this period\n",
    "        test_start_date = Y_pred_ts['ds'].min()\n",
    "        test_end_date = Y_pred_ts['ds'].max()\n",
    "\n",
    "        # Limit historical data to the specified number of days before test_start_date\n",
    "        Y_input_ts = Y_input_ts[(Y_input_ts['ds'] >= history_start_date) & (Y_input_ts['ds'] <= test_start_date)]\n",
    "\n",
    "        # Sort by date\n",
    "        Y_input_ts = Y_input_ts.sort_values('ds')\n",
    "        Y_pred_ts = Y_pred_ts.sort_values('ds')\n",
    "\n",
    "        # Combine historical and prediction data for continuous plotting\n",
    "        combined_ts = pd.concat([Y_input_ts, Y_pred_ts], ignore_index=True)\n",
    "        combined_ts = combined_ts.sort_values('ds')\n",
    "\n",
    "        # Plot the historical data (before the test period)\n",
    "        historical_data = combined_ts[combined_ts['ds'] <= test_start_date]\n",
    "        ax.plot(historical_data['ds'], historical_data['y'], color='blue', linewidth=1)\n",
    "\n",
    "        # Plot the actual data in the test period\n",
    "        actual_test_data = combined_ts[(combined_ts['ds'] >= test_start_date) & (combined_ts['ds'] <= test_end_date)]\n",
    "        ax.plot(actual_test_data['ds'], actual_test_data['y'], color='blue', linewidth=1, linestyle='--')\n",
    "\n",
    "        # Add train data fill (diagonal lines) up to the cutoff_date\n",
    "        train_data = historical_data[historical_data['ds'] <= cutoff_date]\n",
    "        ax.fill_between(train_data['ds'], y_min, y_max,\n",
    "                        facecolor='none', edgecolor=colors['train'], hatch='//', linewidth=0, alpha=0.5, label='Train Data')\n",
    "        \n",
    "        # Fill between for input data (from cutoff_date to test_start_date)\n",
    "        input_data = historical_data\n",
    "        ax.fill_between(input_data['ds'], y_min, y_max,\n",
    "                        facecolor=colors['input'], alpha=0.1, label='Input Data')\n",
    "\n",
    "        # Fill between for test data\n",
    "        ax.fill_between(actual_test_data['ds'], y_min, y_max,\n",
    "                        facecolor=colors['test'], alpha=0.2, label='Test Data')\n",
    "\n",
    "        # Plot the model predictions\n",
    "        for model in model_list:\n",
    "            if model in Y_pred_ts.columns:\n",
    "                color = model_colors.get(model, 'black')\n",
    "                ax.plot(Y_pred_ts['ds'], Y_pred_ts[model], label=model, linewidth=0.9, color=color)\n",
    "            else:\n",
    "                print(f\"Model '{model}' not found in predictions for period '{period}'\")\n",
    "\n",
    "        # Add vertical line to indicate the cutoff date (same for all plots)\n",
    "        ax.axvline(cutoff_date, color='black', linestyle='dashdot', linewidth=0.75)\n",
    "\n",
    "        # Add vertical gray dashed lines at the start and end of the test period\n",
    "        ax.axvline(test_start_date, color='gray', linestyle='--', linewidth=0.75)\n",
    "        ax.axvline(test_end_date, color='gray', linestyle='--', linewidth=0.75)\n",
    "\n",
    "        # Set x and y limits with padding\n",
    "        ax.set_xlim([x_min_padded, x_max_padded])\n",
    "        ax.set_ylim([y_min_padded, y_max_padded])\n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(f'Predictions for Period {i+1}')\n",
    "\n",
    "        # Remove x-axis labels and ticks for all plots except the bottom row\n",
    "        if i < (nrows - 1) * ncols:\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        else:\n",
    "            ax.set_xlabel('Date')\n",
    "\n",
    "        # Remove y-axis labels and ticks for plots on the right side\n",
    "        if ncols > 1 and (i % ncols) == (ncols - 1):\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_yticklabels([])\n",
    "            ax.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "        else:\n",
    "            ax.set_ylabel('Value')\n",
    "\n",
    "        # Collect handles and labels for the legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Filter out unwanted labels\n",
    "        desired_labels = ['Train Data', 'Input Data', 'Test Data'] + model_list\n",
    "        handles_labels = [(h, l) for h, l in zip(handles, labels) if l in desired_labels]\n",
    "\n",
    "        # Update the legend\n",
    "        if handles_labels:\n",
    "            handles, labels = zip(*handles_labels)\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            if period == 'period06':\n",
    "                ax.legend(by_label.values(), by_label.keys(), loc='lower left')\n",
    "            else:\n",
    "                ax.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "    # Remove any empty subplots if periods < total subplots\n",
    "    for j in range(i+1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/figures/period_plot.png', dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot the predictions\n",
    "unique_id = '6'\n",
    "model_list = ['NHITS'] \n",
    "\n",
    "# Choose grid_shape='6x1' or '3x2'\n",
    "plot_model_predictions(unique_id, Y_input_dfs, Y_pred_dfs, model_list, history_days=100, grid_shape='6x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49498292-735d-4a13-ad47-3a2915790017",
   "metadata": {},
   "source": [
    "## 4. In-Sample Evaluation\n",
    "\n",
    "### 4.1 Metrics for the insample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb6763-0acc-4c52-8ed8-a0d813448c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store evaluation dataframes\n",
    "evaluations = {}\n",
    "\n",
    "# Loop over the 6 periods\n",
    "for i in range(6):\n",
    "    print(f\"Calculating metrics for period{i+1:02d}...\")\n",
    "    \n",
    "    # Get the period name\n",
    "    period_name = f'period{i+1:02d}'\n",
    "    \n",
    "    # Get the prediction dataframe for this period\n",
    "    Y_pred_df = Y_pred_dfs[period_name]\n",
    "    \n",
    "    # Get the corresponding test dataframe\n",
    "    Y_test_df = Y_test_dfs[i]\n",
    "    \n",
    "    # Perform evaluation\n",
    "    eval_1_day, eval_7_days, eval_14_days, eval_30_days = ts_utils.perform_evaluation(Y_train_df, Y_test_df, Y_pred_df)\n",
    "\n",
    "    print(f\"Saving metrics for {period_name}..\")\n",
    "    \n",
    "    # Save the evaluation dataframes to CSV files\n",
    "    eval_1_day.to_csv(f\"metrics/insample/{period_name}/metrics_1_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    eval_7_days.to_csv(f\"metrics/insample/{period_name}/metrics_7_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    eval_14_days.to_csv(f\"metrics/insample/{period_name}/metrics_14_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    eval_30_days.to_csv(f\"metrics/insample/{period_name}/metrics_30_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    \n",
    "    # Store the evaluation dataframes in a dictionary\n",
    "    evaluations[period_name] = {\n",
    "        'eval_1_day': eval_1_day,\n",
    "        'eval_7_days': eval_7_days,\n",
    "        'eval_14_days': eval_14_days,\n",
    "        'eval_30_days': eval_30_days\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8afe59-af43-4bbc-8a35-dba3e386f3a1",
   "metadata": {},
   "source": [
    "## 5. Outsample Analysis\n",
    "\n",
    "### 5.1 Train/Test splitting and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e42da-6d14-404b-ad16-df32f56586d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data to the long format\n",
    "Y_df = out_sample_data.melt(id_vars=['date'], var_name='unique_id', value_name='y')\n",
    "Y_df = Y_df.rename(columns={'date':'ds'})\n",
    "\n",
    "# Convert date column to datetime type\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa3eae-0bf5-47ca-a88b-fe7ad271f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the horizon (12 months of 30 days each)\n",
    "fh = 30\n",
    "horizon = 12 * fh\n",
    "\n",
    "# Identify the unique dates in the dataset\n",
    "unique_dates = Y_df['ds'].unique()\n",
    "\n",
    "# Convert to a list and then sort the dates\n",
    "unique_dates = sorted(list(unique_dates))\n",
    "\n",
    "# Determine the cutoff date (cutoff at 12 months before the last date in the dataset)\n",
    "cutoff_date = unique_dates[-(horizon + 1)]\n",
    "\n",
    "# Training data: all data up to the cutoff date\n",
    "Y_train_df = Y_df[Y_df['ds'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92cde9-91bd-42a4-8ec8-f4ac50e86d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the input and test sets\n",
    "input_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Loop to create the 6 input and test sets\n",
    "for i in range(6):\n",
    "    # Determine the start date of the test period\n",
    "    test_start_date = unique_dates[-(horizon - i * 2 * fh)]\n",
    "    test_end_date = unique_dates[-(horizon - (i * 2 * fh) - fh)]\n",
    "    \n",
    "    # Input data: all data up to the start of the current test period\n",
    "    input_df = Y_df[Y_df['ds'] <= test_start_date]\n",
    "    input_dfs.append(input_df)\n",
    "    \n",
    "    # Test data: the 30-day period following the start of the test period\n",
    "    test_df = Y_df[(Y_df['ds'] > test_start_date) & (Y_df['ds'] <= test_end_date)]\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "# Define the 6 input periods\n",
    "Y_input_df_0 = input_dfs[0]\n",
    "Y_input_df_1 = input_dfs[1]\n",
    "Y_input_df_2 = input_dfs[2]\n",
    "Y_input_df_3 = input_dfs[3]\n",
    "Y_input_df_4 = input_dfs[4]\n",
    "Y_input_df_5 = input_dfs[5]\n",
    "\n",
    "# Define the 6 test periods\n",
    "Y_test_df_0 = test_dfs[0]\n",
    "Y_test_df_1 = test_dfs[1]\n",
    "Y_test_df_2 = test_dfs[2]\n",
    "Y_test_df_3 = test_dfs[3]\n",
    "Y_test_df_4 = test_dfs[4]\n",
    "Y_test_df_5 = test_dfs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54668c5b-be33-4d5d-ad07-3a97abc9450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeserie to plot\n",
    "unique_id = Y_input_df_0['unique_id'][0]\n",
    "\n",
    "# Plot the train and test dataframes\n",
    "ts_utils.plot_train_test_split(Y_input_df_0, Y_test_df_0, unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e44de-9af8-4fde-a7d6-748afca19db1",
   "metadata": {},
   "source": [
    "### 5.2 Retrieve Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae126c-360f-4d7a-b070-6509fbedfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and periods\n",
    "models = ['Naive', 'ARIMA', 'ETS', 'NHITS', 'PatchTST', 'TimesNet', 'DeepAR', 'Chronos-small', 'Chronos-large', 'Chronos-FT']\n",
    "periods = ['period01', 'period02', 'period03', 'period04', 'period05', 'period06']\n",
    "\n",
    "# Create Y_test_dfs as a list of test dataframes for each period\n",
    "Y_test_dfs = [Y_test_df_0, Y_test_df_1, Y_test_df_2, Y_test_df_3, Y_test_df_4, Y_test_df_5]\n",
    "\n",
    "# Initialize a dictionary to hold the prediction dataframes for each period\n",
    "Y_pred_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b503a7-2c7b-4715-8055-872acb36e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the predictions\n",
    "def merging_preds(Y_pred_df, model_preds, model_name):\n",
    "    # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "    model_preds['unique_id'] = model_preds['unique_id'].astype('string')\n",
    "    model_preds['ds'] = pd.to_datetime(model_preds['ds'])\n",
    "    \n",
    "    # Merge predictions on 'unique_id' and 'ds'\n",
    "    Y_pred_df = Y_pred_df.merge(model_preds[['unique_id', 'ds', f'{model_name}']], on=['unique_id', 'ds'], how='left')\n",
    "    \n",
    "    return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efc194-513f-48ff-bc2e-8ba2b0ca4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over periods to get predictions\n",
    "for i, period in enumerate(periods):\n",
    "    print(f\"Processing {period}...\")\n",
    "    \n",
    "    # Get the test dataframe for this period\n",
    "    Y_pred_df = Y_test_dfs[i].copy()\n",
    "    \n",
    "    # Ensure 'unique_id' is string and 'ds' is datetime\n",
    "    Y_pred_df['unique_id'] = Y_pred_df['unique_id'].astype('string')\n",
    "    Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "    \n",
    "    # Loop over models to merge predictions\n",
    "    for model in models:\n",
    "        # Read the prediction csv\n",
    "        model_preds = pd.read_csv(f\"predictions/{model}/outsample/{period}/model_preds_{data_date}_{data_size}.csv\")\n",
    "        \n",
    "        # Merge the predictions into Y_pred_df\n",
    "        Y_pred_df = merging_preds(Y_pred_df, model_preds, model)\n",
    "\n",
    "    # Set 'unique_id' as index if needed\n",
    "    Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "    \n",
    "    # Rename columns if necessary\n",
    "    Y_pred_df = Y_pred_df.rename(columns={\"Chronos-small\": \"Chronos (small)\", \"Chronos-large\": \"Chronos (large)\", \"Chronos-FT\": \"Chronos (FT)\"})\n",
    "    \n",
    "    # Store the dataframe in the dictionary\n",
    "    Y_pred_dfs[period] = Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465f7c1-d31a-4246-970f-669342bcf174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_input_dfs as a list of input dataframes for each period\n",
    "Y_input_dfs = [Y_input_df_0, Y_input_df_1, Y_input_df_2, Y_input_df_3, Y_input_df_4, Y_input_df_5]\n",
    "\n",
    "# Function to plot predictions for a given unique_id\n",
    "def plot_model_predictions(unique_id, Y_input_dfs, Y_pred_dfs, model_list, history_days=100, grid_shape='3x2'):\n",
    "    # Parse grid_shape into nrows and ncols\n",
    "    if grid_shape == '3x2':\n",
    "        nrows, ncols = 3, 2\n",
    "    elif grid_shape == '6x1':\n",
    "        nrows, ncols = 6, 1\n",
    "    else:\n",
    "        # Default to 6x1 if invalid input\n",
    "        nrows, ncols = 6, 1\n",
    "\n",
    "    # Prepare to find overall x and y limits\n",
    "    all_dates = []\n",
    "    all_values = []\n",
    "\n",
    "    # Define colors for the different regions\n",
    "    colors = {\n",
    "        'train': '#a6bddb',\n",
    "        'input': '#fd8d3c',\n",
    "        'test': '#feb24c',\n",
    "    }\n",
    "\n",
    "    # Distinct model colors\n",
    "    colors_list = ['purple', 'orange', 'cyan', 'magenta', 'brown', 'red', 'green', 'olive', 'navy', 'teal']\n",
    "    \n",
    "    # Create a mapping from model names to colors\n",
    "    model_colors = dict(zip(model_list, colors_list))\n",
    "\n",
    "    # First, determine the cutoff_date (end of train data)\n",
    "    # We'll use the last date from the first Y_input_df\n",
    "    Y_input_df_first = Y_input_dfs[0].copy()\n",
    "    Y_input_df_first['ds'] = pd.to_datetime(Y_input_df_first['ds'])\n",
    "    if Y_input_df_first.index.name != 'unique_id':\n",
    "        Y_input_df_first = Y_input_df_first.set_index('unique_id')\n",
    "    try:\n",
    "        Y_input_ts_first = Y_input_df_first.loc[unique_id].copy()\n",
    "    except KeyError:\n",
    "        print(f\"unique_id '{unique_id}' not found in the first Y_input_df\")\n",
    "        return\n",
    "    if isinstance(Y_input_ts_first, pd.Series):\n",
    "        Y_input_ts_first = Y_input_ts_first.to_frame().T\n",
    "    cutoff_date = Y_input_ts_first['ds'].max()\n",
    "\n",
    "    # Calculate start date for historical data\n",
    "    history_start_date = cutoff_date - pd.Timedelta(days=history_days)\n",
    "\n",
    "    # Collect all dates and values for axis limits\n",
    "    for i, period in enumerate(periods):\n",
    "        # Get input data for the period\n",
    "        Y_input_df = Y_input_dfs[i].copy()\n",
    "        Y_input_df['ds'] = pd.to_datetime(Y_input_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_input_df.index.name != 'unique_id':\n",
    "            Y_input_df = Y_input_df.set_index('unique_id')\n",
    "\n",
    "        # Get prediction data for the period\n",
    "        Y_pred_df = Y_pred_dfs[period].copy()\n",
    "        Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_pred_df.index.name != 'unique_id':\n",
    "            Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "\n",
    "        # Filter data for the specific unique_id\n",
    "        try:\n",
    "            Y_input_ts = Y_input_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            Y_pred_ts = Y_pred_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # If the result is a Series (only one entry), convert it to DataFrame\n",
    "        if isinstance(Y_input_ts, pd.Series):\n",
    "            Y_input_ts = Y_input_ts.to_frame().T\n",
    "\n",
    "        if isinstance(Y_pred_ts, pd.Series):\n",
    "            Y_pred_ts = Y_pred_ts.to_frame().T\n",
    "\n",
    "        # Determine the start and end date of the test period for this period\n",
    "        test_start_date = Y_pred_ts['ds'].min()\n",
    "        test_end_date = Y_pred_ts['ds'].max()\n",
    "\n",
    "        # Limit historical data to the specified number of days before test_start_date\n",
    "        Y_input_ts = Y_input_ts[(Y_input_ts['ds'] >= history_start_date) & (Y_input_ts['ds'] <= test_start_date)]\n",
    "\n",
    "        # Collect dates and values\n",
    "        all_dates.extend(Y_input_ts['ds'].tolist())\n",
    "        all_values.extend(Y_input_ts['y'].tolist())\n",
    "        all_dates.extend(Y_pred_ts['ds'].tolist())\n",
    "        all_values.extend(Y_pred_ts['y'].tolist())\n",
    "\n",
    "        # Collect values from model predictions\n",
    "        for model in model_list:\n",
    "            if model in Y_pred_ts.columns:\n",
    "                all_values.extend(Y_pred_ts[model].tolist())\n",
    "\n",
    "    # Determine overall x and y limits with padding\n",
    "    x_min = min(all_dates)\n",
    "    x_max = max(all_dates)\n",
    "    y_min = min(all_values)\n",
    "    y_max = max(all_values)\n",
    "\n",
    "    # Calculate padding\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_padding = x_range * 0.05  # 5% padding on x-axis\n",
    "    y_padding = y_range * 0.05  # 5% padding on y-axis\n",
    "\n",
    "    # Adjust x_min and x_max with padding\n",
    "    x_min_padded = x_min - x_padding\n",
    "    x_max_padded = x_max + x_padding\n",
    "\n",
    "    # Adjust y_min and y_max with padding\n",
    "    y_min_padded = y_min - y_padding\n",
    "    y_max_padded = y_max + y_padding\n",
    "\n",
    "    # Create the grid of subplots based on grid_shape\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(18, 5 * nrows))\n",
    "    axs = axs.flatten()  # Flatten to easily index subplots\n",
    "\n",
    "    for i, period in enumerate(periods):\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Get input data for the period\n",
    "        Y_input_df = Y_input_dfs[i].copy()\n",
    "        Y_input_df['ds'] = pd.to_datetime(Y_input_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_input_df.index.name != 'unique_id':\n",
    "            Y_input_df = Y_input_df.set_index('unique_id')\n",
    "\n",
    "        # Get prediction data for the period\n",
    "        Y_pred_df = Y_pred_dfs[period].copy()\n",
    "        Y_pred_df['ds'] = pd.to_datetime(Y_pred_df['ds'])\n",
    "\n",
    "        # Ensure 'unique_id' is the index\n",
    "        if Y_pred_df.index.name != 'unique_id':\n",
    "            Y_pred_df = Y_pred_df.set_index('unique_id')\n",
    "\n",
    "        # Filter data for the specific unique_id\n",
    "        try:\n",
    "            Y_input_ts = Y_input_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            print(f\"unique_id '{unique_id}' not found in Y_input_df for period '{period}'\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            Y_pred_ts = Y_pred_df.loc[unique_id].copy()\n",
    "        except KeyError:\n",
    "            print(f\"unique_id '{unique_id}' not found in Y_pred_df for period '{period}'\")\n",
    "            continue\n",
    "\n",
    "        # If the result is a Series (only one entry), convert it to DataFrame\n",
    "        if isinstance(Y_input_ts, pd.Series):\n",
    "            Y_input_ts = Y_input_ts.to_frame().T\n",
    "\n",
    "        if isinstance(Y_pred_ts, pd.Series):\n",
    "            Y_pred_ts = Y_pred_ts.to_frame().T\n",
    "\n",
    "        # Determine the start and end date of the test period for this period\n",
    "        test_start_date = Y_pred_ts['ds'].min()\n",
    "        test_end_date = Y_pred_ts['ds'].max()\n",
    "\n",
    "        # Limit historical data to the specified number of days before test_start_date\n",
    "        Y_input_ts = Y_input_ts[(Y_input_ts['ds'] >= history_start_date) & (Y_input_ts['ds'] <= test_start_date)]\n",
    "\n",
    "        # Sort by date\n",
    "        Y_input_ts = Y_input_ts.sort_values('ds')\n",
    "        Y_pred_ts = Y_pred_ts.sort_values('ds')\n",
    "\n",
    "        # Combine historical and prediction data for continuous plotting\n",
    "        combined_ts = pd.concat([Y_input_ts, Y_pred_ts], ignore_index=True)\n",
    "        combined_ts = combined_ts.sort_values('ds')\n",
    "\n",
    "        # Plot the historical data (before the test period)\n",
    "        historical_data = combined_ts[combined_ts['ds'] <= test_start_date]\n",
    "        ax.plot(historical_data['ds'], historical_data['y'], color='blue', linewidth=1)\n",
    "\n",
    "        # Plot the actual data in the test period\n",
    "        actual_test_data = combined_ts[(combined_ts['ds'] >= test_start_date) & (combined_ts['ds'] <= test_end_date)]\n",
    "        ax.plot(actual_test_data['ds'], actual_test_data['y'], color='blue', linewidth=1, linestyle='--')\n",
    "\n",
    "        # Add train data fill (diagonal lines) up to the cutoff_date\n",
    "        train_data = historical_data[historical_data['ds'] <= cutoff_date]\n",
    "        ax.fill_between(train_data['ds'], y_min, y_max,\n",
    "                        facecolor='none', edgecolor=colors['train'], hatch='//', linewidth=0, alpha=0.5, label='Train Data')\n",
    "        \n",
    "        # Fill between for input data (from cutoff_date to test_start_date)\n",
    "        input_data = historical_data\n",
    "        ax.fill_between(input_data['ds'], y_min, y_max,\n",
    "                        facecolor=colors['input'], alpha=0.1, label='Input Data')\n",
    "\n",
    "        # Fill between for test data\n",
    "        ax.fill_between(actual_test_data['ds'], y_min, y_max,\n",
    "                        facecolor=colors['test'], alpha=0.2, label='Test Data')\n",
    "\n",
    "        # Plot the model predictions\n",
    "        for model in model_list:\n",
    "            if model in Y_pred_ts.columns:\n",
    "                color = model_colors.get(model, 'black')\n",
    "                ax.plot(Y_pred_ts['ds'], Y_pred_ts[model], label=model, linewidth=0.9, color=color)\n",
    "            else:\n",
    "                print(f\"Model '{model}' not found in predictions for period '{period}'\")\n",
    "\n",
    "        # Add vertical line to indicate the cutoff date (same for all plots)\n",
    "        ax.axvline(cutoff_date, color='black', linestyle='dashdot', linewidth=0.75)\n",
    "\n",
    "        # Add vertical gray dashed lines at the start and end of the test period\n",
    "        ax.axvline(test_start_date, color='gray', linestyle='--', linewidth=0.75)\n",
    "        ax.axvline(test_end_date, color='gray', linestyle='--', linewidth=0.75)\n",
    "\n",
    "        # Set x and y limits with padding\n",
    "        ax.set_xlim([x_min_padded, x_max_padded])\n",
    "        ax.set_ylim([y_min_padded, y_max_padded])\n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(f'Predictions for Period {i+1}')\n",
    "\n",
    "        # Remove x-axis labels and ticks for all plots except the bottom row\n",
    "        if i < (nrows - 1) * ncols:\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        else:\n",
    "            ax.set_xlabel('Date')\n",
    "\n",
    "        # Remove y-axis labels and ticks for plots on the right side\n",
    "        if ncols > 1 and (i % ncols) == (ncols - 1):\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_yticklabels([])\n",
    "            ax.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "        else:\n",
    "            ax.set_ylabel('Value')\n",
    "\n",
    "        # Collect handles and labels for the legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Filter out unwanted labels\n",
    "        desired_labels = ['Train Data', 'Input Data', 'Test Data'] + model_list\n",
    "        handles_labels = [(h, l) for h, l in zip(handles, labels) if l in desired_labels]\n",
    "\n",
    "        # Update the legend\n",
    "        if handles_labels:\n",
    "            handles, labels = zip(*handles_labels)\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            if period == 'period06':\n",
    "                ax.legend(by_label.values(), by_label.keys(), loc='lower left')\n",
    "            else:\n",
    "                ax.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "    # Remove any empty subplots if periods < total subplots\n",
    "    for j in range(i+1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/figures/period_plot.png', dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create the plot\n",
    "unique_id = Y_input_df_0['unique_id'][0]  \n",
    "model_list = ['PatchTST']\n",
    "\n",
    "# Choose grid_shape='6x1' or '3x2'\n",
    "plot_model_predictions(unique_id, Y_input_dfs, Y_pred_dfs, model_list, history_days=100, grid_shape='6x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927e94a-42ea-452d-882f-c87fdad4cb21",
   "metadata": {},
   "source": [
    "## 6. Out-Sample Evaluation\n",
    "\n",
    "### 6.1 Metrics for the outsample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84ecd4-9d2e-4f52-ae2a-e373cbc7c738",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-10-30T17:00:35.400342Z",
     "shell.execute_reply": "2024-10-30T17:00:35.399327Z",
     "shell.execute_reply.started": "2024-10-30T16:59:04.458061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store evaluation dataframes\n",
    "evaluations = {}\n",
    "\n",
    "# Loop over the 6 periods\n",
    "for i in range(6):\n",
    "    print(f\"Calculating metrics for period{i+1:02d}...\")\n",
    "    \n",
    "    # Get the period name\n",
    "    period_name = f'period{i+1:02d}'\n",
    "    \n",
    "    # Get the prediction dataframe for this period\n",
    "    Y_pred_df = Y_pred_dfs[period_name]\n",
    "    \n",
    "    # Get the corresponding test dataframe\n",
    "    Y_test_df = Y_test_dfs[i]\n",
    "    \n",
    "    # Perform evaluation\n",
    "    eval_1_day, eval_7_days, eval_14_days, eval_30_days = ts_utils.perform_evaluation(Y_train_df, Y_test_df, Y_pred_df)\n",
    "\n",
    "    print(f\"Saving metrics for {period_name}..\")\n",
    "    \n",
    "    # Save the evaluation dataframes to CSV files\n",
    "    eval_1_day.to_csv(f\"metrics/outsample/{period_name}/metrics_1_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    eval_7_days.to_csv(f\"metrics/outsample/{period_name}/metrics_7_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    eval_14_days.to_csv(f\"metrics/outsample/{period_name}/metrics_14_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    eval_30_days.to_csv(f\"metrics/outsample/{period_name}/metrics_30_day_{data_date}_{data_size}.csv\", index=False)\n",
    "    \n",
    "    # Store the evaluation dataframes in a dictionary\n",
    "    evaluations[period_name] = {\n",
    "        'eval_1_day': eval_1_day,\n",
    "        'eval_7_days': eval_7_days,\n",
    "        'eval_14_days': eval_14_days,\n",
    "        'eval_30_days': eval_30_days\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84a221-0a28-49d6-acd8-f15e5917f46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd1fba-29e4-4e17-b332-87712ff0e1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAP Environment",
   "language": "python",
   "name": "dap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
